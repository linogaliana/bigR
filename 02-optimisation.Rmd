# (PART) Approches in-memory {-}

# Fondamentaux de l'optimisation en R

<script src="./hideOutput.js"></script>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)
```


>  Premature optimization is the root of all evil (or at least most of it) in programming.
>
> Donald Knuth


```{r check reticulate, echo=FALSE}
library(reticulate)

#if (Sys.getenv("LOCAL")==""){
  # CHANGE HERE TO YOUR PATH
Sys.setenv(RETICULATE_PYTHON = "/anaconda3/bin/python3.6")

# CREATE A CONDA ENVIRONMENT JUST FOR RETICULATE
#reticulate::use_condaenv("Reticulate")

#reticulate::conda_install("Reticulate", "scipy")
#}
```

## Outils utiles pour mesurer l'exécution de fonctions

Nous allons utiliser deux types d'approches pour minuter la vitesse d'exécution d'un programme:

* **Benchmarking**: test de performance basé sur la répétition d'opérations;
* **Profiling**: exécution d'une série de commandes afin de déterminer les goulets d'étranglement.

On utilise généralement la première approche lorsqu'on veut optimiser une opération pour laquelle plusieurs méthodes existent. La seconde approche permet de cibler les étapes d'un programme à cibler pour accélérer les calculs ou réduire l'utilisation de la mémoire.

L'utilisation de la CPU et de la RAM peut être contrôlée en temps réel avec le gestionnaire des tâches en Windows ou la commande `htop` sur `Linux`. Avec ces outils, on peut contrôler les processus en cours, l'utilisation des différents coeurs d'un ordinateur, la RAM utilisée et celle disponible. 


### Benchmarking avec `microbenchmark`

Lorsque plusieurs méthodes existent pour réaliser la même opération, une première optimisation consiste à choisir la méthode la plus rapide. Pour ce faire, le temps d'exécution d'une ou plusieurs fonctions peut être mesuré très simplement avec la fonction de base `system.time()`. Cependant, le temps d'exécution observé n'est qu'une réalisation d'une variable aléatoire et dépend de l'utilisation des ressources d'un ordinateur au moment de l'exécution de la fonction. Pour obtenir une estimation plus fiable de la vitesse d'exécution d'une fonction, et être en mesure de comparer de manière plus adéquate des temps d'exécution, le package `microbenchmark` est très pratique. Comme les fonctions évaluées sont répétées de nombreuses fois (100 fois par défaut), on utilise `microbenchmark` pour des opérations rapides (quitte à évaluer les fonctions sur un échantillon plus petit de données). 

Par exemple, si on désire évaluer la performance des moyennes par groupe avec `dplyr`, `data.table` et en base `R`:

<!-----
Exercice: Utiliser le package microbenchmark pour faire une moyenne par groupe du dataframe suivant

1. Avec une solution `R` base:  àggregate`
2. Avec une solution `dplyr`: `group_by` puis `summarise`
3. Avec une solution `data.table`: `dt[,mean(),by = ]`
----->

```{r first microbenchmark example, message = FALSE}
# On importe seulement le pipe, pas tout dplyr
import::from("magrittr","%>%")

# On crée un dataframe avec 5 catégories
df <- data.frame(x = rnorm(10e5),
                 y = sample(1:5,size = 10e5,
                            replace = TRUE))
# On crée son alter-ego data.table pour éviter de faire la conversion N fois
dt <- data.table::data.table(df)
# On indexe le data.table pour tirer profiter pleinement de la performance du package (cf. chapitre data.table)
data.table::setkeyv(dt,"y")



# Compare data.table, dplyr, base R
micro <- microbenchmark::microbenchmark(
  aggregate(df$x, by = list(df$y), FUN = mean),
  dt[,mean(x), by = y],
  df %>% dplyr::group_by(y) %>% dplyr::summarise(mean(x)),
  times = 20
)
print(micro)

ggplot2::autoplot(micro)
```

Chaque ligne correspond à une instruction répétée `n` fois. Des statistiques descriptives du temps d'exécution (par exemple en millisecondes) sont présentées. On peut également afficher le résultat sous forme de *violin plot* en utilisant `ggplot2::autoplot` (par défaut, l'échelle de temps est logarithmique donc les différences sont relativement écrasées). Ici, on voit que l'approche `data.table` est plus rapide

Nous reviendrons sur ce code dans le chapitre `data.table`. Ce benchmark nous indique déjà que l'approche `data.table` est plus rapide (l'ordre de grandeur est ici de l'ordre de 1 pour 4 par rapport à `dplyr` et 1 pour 40 par rapport à la solution R base.)

##### Profiling avec `profvis`

Le *profiling* est une approche plus avancée qui consiste à contrôler le temps d'exécution et l'usage mémoire d'une série de fonction. C'est une approche particulièrement bien adaptée à des codes modulaires (découpés sous formes de morceaux appelés *chunks*) car elle permet d'isoler les parties de code les plus gourmandes en temps ou en mémoire. Nous aurons l'occasion, dans les exercices, d'appliquer plusieurs fois la fonction `profvis` du package du même nom.

```{r, eval = TRUE}
profvis::profvis(expr = {
  
  # Stage 1: load packages
  library("magrittr")
  
  # Stage 2: load and process data
  df <- data.frame(x = rnorm(10e5),
                    y = sample(1:5,size = 10e5, replace = TRUE))
  
  df2 <- df %>% dplyr::group_by(y) %>%
    dplyr::summarise(x2 = mean(x)) %>%
    dplyr::mutate(x2 = x2/mean(x2))
  
  df <- df %>% dplyr::left_join(df2)

  # Imaginons on fait une moyenne par groupe avec lapply
  # (nb: pas du tout la methode la plus efficace)
  # lapply(unique(df$y))

  # Stage 3: visualise output
  p <- ggplot2::ggplot(df,
                  ggplot2::aes(x = factor(y), y = x)
                  ) +
    ggplot2::geom_boxplot(ggplot2::aes(colour = factor(y))) +
    ggplot2::theme_bw()
  
  print(p)
  
})


```

Ce package offre une belle représentation du temps passé et des usages mémoires pour chaque bout de programme. C'est particulièrement utile pour déterminer les goulets d'étranglement, et donc les morceaux à optimiser, d'un programme. Avec cet exemple, on voit que la majorité du temps est passée dans la représentation graphique, étape la plus longue et la plus exigeante en mémoire. 

CommOM: il me semble indispensable de bien expliquer comment se lisent les résultats. Personnellement je ne comprends pas grand-chose, et je pense que profvis peut être très utile à beaucoup d'utilisateurs.

**Exercice:**

Faire un `microbenchmark` d'une somme cumulative avec:

1. Une boucle `for`;
2. Une solution utilisant `sapply`;
3. La fonction `cumsum`.

Vous pouvez créer un vecteur `x <- seq_len(100)` pour faire ce test.

<div class="fold s">
```{r}
x <- seq_len(100) # initiate vector to cumulatively sum

# Method 1: with a for loop (10 lines)
cs_for = function(x){
  for(i in x){
    if(i == 1){
      xc = x[i]
    } else {
      xc = c(xc, sum(x[1:i]))
    }
  }
  xc
}

# Method 2: with apply (3 lines)
cs_apply = function(x){
  sapply(x, function(x) sum(1:x))
}

# Method 3: cumsum (1 line, not shown)
print(
  microbenchmark::microbenchmark(cs_for(x), cs_apply(x), cumsum(x))
)

```
</div>

Vous pouvez apprécier la différence de vitesse entre un code vectorisé (`cumsum`) et des approches plus artisanales, qui sont également moins lisibles. 

### Comparer la performance de R par rapport à d'autres utilisateurs


Le package `benchmarkme` est conçu pour faire tourner quelques calculs standardisés afin de comparer la performance de `R` à d'autres utilisateurs qui ont accepté de partager la performance de leur ordinateur. 

```{r, eval = FALSE}
res = benchmarkme::benchmark_std() 
```

Pour se comparer aux autres utilisateurs: 

```{r, eval = FALSE}
plot(res)
```

![](./pics/benchmarkme1.png)
![](./pics/benchmarkme2.png)
![](./pics/benchmarkme3.png)

Le poste utilisé pour les tests a des performances médianes: sans être catastrophique, il n'est pas non plus exceptionnel.


## Comprendre les ordinateurs pour comprendre `R`

Un ordinateur est une réalisation concrète d’une machine de Turing, c'est-à-dire une
machine traitant des informations et capable en principe de prendre comme donné n’importe quel algorithme et de l’exécuter.

### Principes généraux

Les deux principaux constituants d’un ordinateur sont la **mémoire principale** et le **processeur** ou **CPU** (*Central Processing Unit*). La mémoire principale permet de stocker de l’information (programmes et données), tandis que le processeur exécute pas à pas les instructions composant les programmes.

<!---CommOM: il n'y a pas les bus sur le schéma.---->

L'interaction entre les différentes composantes d'un ordinateur suit l’architecture de Von-Neumann:

1. Le <span style="color:royalblue">**processeur (CPU pour Central Processing Unit)**</span> est conceptuellement décomposé en:
    + une *unitée de contrôle (UC)*: séquence les opérations;
    + une *unitée de calcul arithmétique et logique (UAL)*: exécute des opérations de base;
    + un *registre*.
2. La <span style="color:royalblue">**mémoire**</span>: contient à la fois les données et le programme
exécuté par l’unité de contrôle:
    + *Mémoire vive ou volatile (RAM pour random access memory)*: contient les programmes et les données en cours de traitement;
    + *Mémoire permanente (ROM pour read only memory)*: stocke les programmes et les données de base de la machine.
3. <span style="color:royalblue">**Dispositifs d'entrée-sortie (input/output)**</span>: périphériques qui permettent de communiquer avec le monde extérieur.
4. <span style="color:royalblue">**Canaux de communication (les bus)**</span> entre la mémoire, le processeur et les périphériques.


![](./pics/architecture_von_neumann.png)

La **carte-mère** est le support physique permettant la coordination de l’ensemble des éléments intervenant dans le fonctionnement d’un ordinateur.

Si l’architecture de Von-Neumann se caractérise par sa grande souplesse, elle présente en revanche trois gros inconvénients :

1. Une *exécution séquentielle*. L’architecture de Von Neumann impose une exécution séquentielle des instructions. Avec ce modèle, il n’est donc pas possible d’effectuer différents travaux en parallèle sur un même processus.
2. Un *goulet d’étranglement*. Alors que le microprocesseur peut exécuter des instructions très rapidement, celles-ci sont de toute façon ralenties par la vitesse de transfert des informations dans les bus de communication.
3. *Faible robustesse*. Le fait que les données et les programmes soient mélangés engendre une fragilité du modéle de Von-Neumann : si une donnée est enregistrée à l’emplacement d’un programme, le comportement de l’ordinateur peut devenir complètement incohérent.

Le **système d'exploitation** dirige l'utilisation des ressources d'un ordinateur. Cela donne l'illusion que de nombreux processus peuvent être exécutés en même temps. Cela provient de la conjonction de deux effets : d’une part, les instructions exécutées par l’ordinateur sont extrêmement rapides (de l’ordre de quelques dizaines de millisecondes) et d’autre part, le système d’exploitation utilise les temps d’attente dans l’exécution d’une tâche pour orienter les ressources de l’ordinateur (micro-processeur + RAM...) vers l’exécution d’une autre tâche. Le fait que la plupart des ordinateurs soient maintenant dotés de plusieurs processeurs permet, si les applications sont adaptées, d’exécuter réellement plusieurs tâches simultanément. Malgré tout, le nombre d’instructions effectuées simultanément reste habituellement très inférieur au nombre d’applications en fonctionnement simultané.

#### Langages interprétés et langages compilés

On distingue généralement:

* langage compilé: langage qui est traduit en langage machine (binaire) avant de pouvoir être exécuté. C'est directement le système d'exploitation qui va utiliser le code binaire 
* langage interprété: instructions sont interprétées au fur et à mesure. L'interpréteur va exécuter les lignes du code une par une, en décidant à chaque étape ce qu'il va faire ensuite. 

`R` est un langage interprété (comme `python`), par opposition aux langages compilés (`C++` par exemple). Par rapport à l’interpréteur, le compilateur présente l’inconvénient de retarder l’exécution d’un programme mais une fois compilé, le fichier exécutable obtenu est particulièrement rapide (moins de surcouche vers le langage machine).

Certains packages (par exemple `compiler`) proposent d'accélérer les traitements en transformant des exécutions interprétées (du code `R` standard) en instructions compilées (`bytecode`). C'est une méthode, assez simple puisque c'est le package qui gère cette traduction, pour accélérer certains programmes. Depuis `R 3.5.0` (avril 2018), les fonctions intégrées à des packages sont automatiquement compilées, ce qui accélère les temps d'exécution sans effort de programmation. 


### Le processeur ou CPU (*Central Processing Unit*)


Le processeur (ou micro-processeur) est considéré comme le coeur de l’ordinateur. C’est lui qui attribue des tâches simples aux périphériques et à la RAM tandis qu’il exécute les tâches les plus compliquées. Il est responsable de l’exécution d’un programme. Le processeur est un circuit éléctronique complexe (circuit intégré) qui exécute chaque instruction très rapidement, en quelques **cycles d’horloges**. Toute l’activité de l’ordinateur est cadencée par une horloge unique, de façon à ce que tous les circuits électroniques travaillent tous ensemble de façon synchronisée. La fréquence de cette horloge s’exprime en `MHz` (millions de cyles par seconde) ou `GHz` (milliards de cycles par secondes). Par exemple, le processeur *Intel Core i5-6300U* qui équipe les postes nomades Insee possède une horloge cadencée à 2,40 GHz.

Un processeur est défini, entre autres, par:

1. La largeur de ses registres internes de manipulation de données. Les registres sont des mémoires de petite taille (quelques octets), suffisamment rapides pour que l’UAL puisse manipuler leur contenu à chaque cycle de l’horloge. Les deux largeurs les plus communes sont 32 et 64 bits;
2. La cadence de son horloge exprimée en MHz ou GHz ;
3. Le nombre de noyaux de calcul (*cores*). Le chapitre sur la parallélisation reviendra sur ce point.

Le processeur est principalement divisé en deux parties:

1. L’unité de contrôle: responsable de la lecture en mémoire principale et du décodage des instructions ;
2. L’unité de calcul arithmétique et logique (UAL): exécute les instructions qui manipulent les données. C’est elle qui effectue les opérations algébriques (somme, différence, produit, rapport) et logiques (disjonction, conjonction, négation) usuelles.

Ces deux unités communiquent avec la mémoire principale, la première pour lire les instructions, la seconde pour recevoir/transmettre des données binaires, mais ils communiquent également avec les différents périphériques (clavier, souris, écran, etc.).


<!--------
Les instructions que doit suivre l’unité de contrôle sont elles aussi inscrites dans la RAM. Pour les exécuter, l’unité de contrôle utilise un registre particulier appelé PC (Program Counter) et exécute en boucle la séquence d’actions suivantes :

  * Lire instruction : Aller lire le mot stocké à l’adresse mémoire inscrite dans le registre PC et le stocker dans un registre spécial appelé IR (Registre d’Instruction). Les instructions sont des mots mémoires codés en
binaire que l’on appelle le langage machine.
  * Incrémenter PC : Ajouter 1 au mot stocké dans le registre PC et enregistrer le résultat dans ce même registre.
  * Décoder instruction : Décoder l’instruction contenue dans IR, c’est à dire, reconnaître s’il s’agit d’une
opération arithmétique et logique, d’un accès à la mémoire vive ou d’un branchement (voir plus loin).
  * Exécuter instruction : Exécuter l’instuction décodée.
  
Le programme est représenté par une série d’instructions qui réalisent
des opérations en liaison avec la mémoire vive de l’ordinateur. Il y a
quatre étapes lors du traitement des instructions (architecture de Von Neumann):
1 FETCH : Recherche de l’instruction ;
2 DECODE : Décodage de l’instruction ;
3 EXECUTE : Exécution des opérations ;
4 WRITEBACK : Écriture du résultats.
  
------>

### La mémoire vive ou RAM (*random access memory*)

La mémoire est divisée en bytes qui sont des emplacements (des cases mémoires contiguës) de taille fixe utilisés pour stocker instructions et données. Il est facile de confondre les termes `bit` et `byte`: un `bit` (contraction de *binary digit*) est l'élement de base de l'ordinateur, le `byte` stocke plusieurs éléments.
En principe, la taille d’un emplacement mémoire pourrait être quelconque ; en pratique, la plupart des ordinateurs en service aujourd’hui utilisent des emplacements mémoire d’un octet (soit 8 bits)^[Aujourd'hui, les termes bytes et octets sont utilisés de manière relativement indistincte même si, conceptuellement, il s'agit de notions différentes. Le byte est le nombre de bits nécessaire pour coder un caractère. Il s'agit ainsi de la plus petite unité logique adressable par un programme sur un ordinateur. Les bytes de 8 bits se sont généralisés dans les ordinateurs modernes même si, conceptuellement, les bytes pouvaient être de longueur différente. Par le passé, les caractères ASCII dominaient l'informatique. Il s'agissait d'un ensemble de 128 caractères qui ne nécessitaient que 7 bits ($2^7=128$) mais en utilisaient 8 pour optimiser la performance. La définition d'un octet est d'être une unité composée de 8 bits. C'est la généralisation des bytes à 8 bits qui explique la correspondance entre ces deux termes. En informatique, si l'on veut explicitement désigner une quantité de huit bits, on utilise le mot octet ; tandis que si l'on veut exprimer l'unité d'adressage indépendamment du nombre de bits, on utilise le mot byte.]. Les séquences de bytes sont elles-mêmes regroupées en mots mémoire dont la taille est déterminée par celle des registres de l'ordinateur (32 ou 64 bits)^[Les processeurs 32 bits ne peuvent normalement pas adresser plus de 4 Go ($2^{32}$ octets) de mémoire centrale, tandis que les processeurs 64 bits peuvent en adresser 16 Eo ($2^{64}$ octets). C'est pourquoi dès qu'il y a plus de 4 Go de RAM sur une machine, la mémoire au-delà de ce seuil ne sera directement adressable qu'en mode 64 bits].

<!---Il existe au moins 3 types de mémoire vive : SD, SSD et DDR-SDRAM (d’accès plus rapide).---->


Bit representation | Caractère
-------------------|-----------
01000001           | A
01000010           | B
01000011           | C



La RAM possède les caractéristiques suivantes :

1. Un mot dans la mémoire peut aussi bien représenter une instruction dans un programme, qu’un entier ou une couleur selon l’interprétation que lui donne l’utilisateur;
2. Elle est inerte dans le sens où elle sert juste de support de stockage de l’information;
3. Tout mot mémoire possède une adresse (codé sous la forme d’un entier) lui permettant d’être lu rapidement par le microprocesseur.

Lorsque la mémoire vive arrive à saturation en raison d’une utilisation instantanée intensive de l’ordinateur, une partie du disque dur peut être utilisée par le microprocesseur : on appelle cela le *swap*. Dans ce cas, le temps d’exécution des instructions est beaucoup plus lent (de l’ordre de 1000 fois).

<!----------
Seul le processeur peut modifier l’état de la mémoire. Chaque emplacement mémoire conserve les informations que le processeur y écrit jusqu’à coupure de l’alimentation électrique, où tout le contenu est perdu (contrairement au contenu des mémoires externes comme
les disquettes et disques durs). On parle de mémoire vive. Les seules
opérations possibles sur la mémoire sont :
1 écriture d’un emplacement : le processeur donne une valeur et
une adresse, et la mémoire range la valeur à l’emplacement
indiqué par l’adresse ;
2 lecture d’un emplacement : le processeur demande à la mémoire
la valeur contenue à l’emplacement dont il indique l’adresse. Le
contenu de l’emplacement auquel le processeur accède en lecture
demeure inchangé.

Caractéristiques d’une mémoire
1 La capacité : nombre total de bits que contient la mémoire. Elle
s’exprime aussi souvent en octet ;
2 Le format des données : nombre de bits que l’on peut mémoriser
par case mémoire. On parle de la largeur du mot mémorisable ;
3 Le temps d’accès : temps qui s’écoule entre l’instant où a été
lancée une opération de lecture/écriture en mémoire et l’instant
où la première information est disponible sur le bus de données ;
4 Le temps de cycle : il représente l’intervalle minimum qui doit
séparer deux demandes successives de lecture ou d’écriture ;
5 Le débit : nombre maximum d’informations lues ou écrites par
seconde ;
6 La volatilité : elle caractérise la permanence des informations
dans la mémoire. L’information stockée est volatile si elle risque
d’être altérée par un défaut d’alimentation électrique et non
volatile dans le cas contraire.
------------>


<!-------------
Les instructions et les données transmises au processeur sont
exprimées en mots binaires (code machine). Elles sont stockées dans
la mémoire. Le séquenceur ordonne la lecture du contenu de la
mémoire et la constitution des mots présentées à l’UAL qui les
interprète. L’ensemble des instructions et des données constitue un
programme. Le langage le plus proche du code machine tout en
restant lisible par des humains est le langage d’assemblage, aussi
appelé langage assembleur
------------------>


## Retour à `R`: appels de fonctions

> To understand computations in R, two slogans are helpful:
>
>  * Everything that exists is an object.
>  * Everything that happens is a function call.
>
> *John Chambers*

Appeler une fonction `R` revient en fait à appeler, de manière plus ou moins directe, une fonction codée en `C` ou en `Fortran`. Par exemple, la fonction `runif()` contient une unique ligne qui est un appel à la fonction  `C_runif()`.

```{r runif C call}
body(runif)
```

Les fonctions développées en `C++` grâce au package `Rcpp` fonctionnent de la même manière. Par exemple, 
```{r}
dplyr::between
```
fonctionne comme un *wrapper* d'une fonction sous-jacente en `C++` qui est, elle-même, stockée dans un fichier compilé `.dll` (sous windows)

```{r}
dplyr:::`dplyr_between`
```

Pour obtenir de bonnes performances en `R`, il est important d'accéder aux routines sous-jacentes `C` ou `Fortran` le plus rapidement possible. Dans la mesure où appeler une fonction a un coût en terme de temps d'exécution, il est important de minimiser autant que possible le nombre d'appels pour accéder aux fonctions `C` ou `Fortran`.

Ce principe explique la lenteur des boucles en `R`. Par exemple, en supposant que `x` soit un vecteur standard, 

```{r, eval = FALSE}
# ex: x <- runif(10)
x = x + 1
```

ne nécessite qu'un appel à la fonction `+`. 

La boucle `for` équivalente

```{r for loop, eval = FALSE}
for(i in seq_len(n)) 
  x[i] <- x[i] + 1 
```

nécessite

* *n* appels à la fonction `+` ;
* *n* appels à la fonction `[` ;
* *n* appels à la fonction `[<-` (assignation);
* 2 appels supplémentaires: un à la fonction `for` et l'autre `seq_len()`.

La boucle *for* n'est pas longue en soi, mais les appels de fonctions sont bien trop nombreux pour la rendre compétitive. En effet, les appels de fonctions sous `R` impliquent des sur-couches par rapport à un langage compilé comme `C` ou `C++` qui accède plus directement au langage machine. En particulier, l'exécution d'une fonction implique de créer un nouvel environnement.
Les méthodes de compilation à la volée (*just-in-time compilation*) telles qu'implémentées par le package `compiler` permettent d'accélérer les boucles mais il reste toujours préférable d'adopter une approche plus adaptée à `R` ou, si c'est impossible, de privilégier une boucle `C++` [@eddelbuettel2011rcpp].



<!---- Comme nous le montrerons par la suite, si la vectorisation n'est pas possible, il est préférable d'utiliser la fonction `lapply` que la fonction `for`.---->


**Exercice**

Utilisez le package `microbenchmark` pour comparer la solution de l'addition vectorielle à la version boucle. Faites la comparaison en variant les tailles de vecteur (conseil: créez deux fonctions `plus_one` et `plus_one_for` prenant comme paramètre `n` qui est utilisé pour générer une variable aléatoire de taille `n`)

<div class="fold s">
```{r microbenchmark vectorisation}
plus_one <- function(n){
  x <- runif(n)
  x <- x+1
  return("OK")
}

plus_one_for <- function(n){
  
  x <- runif(n)
  for(i in seq_len(n)) x[i] = x[i] + 1 
  return("OK")
  
  return(x)
}

results_plus_one <- microbenchmark::microbenchmark(
  plus_one(10^2),
  plus_one(10^3),
  plus_one(10^4),
  plus_one(10^5),
  plus_one_for(10^2),
  plus_one_for(10^3),
  plus_one_for(10^4),
  plus_one_for(10^5),
  times = 50
  )

ggplot2::autoplot(results_plus_one)
```
</div>

## Mémoire

**`R` est un langage qui a une utilisation intensive de la RAM**. C'est ce qui permet au langage d'être assez rapide mais crée des difficultés lorsque les données sont volumineuses et/ou lorsqu'on travaille dans un environnement où la RAM est contrainte (les postes nomades de l'Insee ont 8Go de RAM; les ressources d'un serveur sont partagées entre tous les utilisateurs). Dans cette partie, nous verrons qu'outre l'import de données, la gourmandise de `R` provient de la manière dont les objets sont copiés en mémoire lorsqu'ils sont modifiés.

Le livre de @wickham2014advanced est une ressource précieuse sur le sujet. De nombreux exemples de cette section sont tirés de cet ouvrage. On utilisera le package `pryr` en supplément de `profvis` pour contrôler l'usage mémoire.

### Bases de la gestion de la mémoire en `R`

Pour comprendre les besoins mémoire de `R`, on peut exécuter le code suivant qui inspecte la taille mémoire d'un vecteur en fonction de sa longueur:

```{r}
sizes <- sapply(0:50, function(n) pryr::object_size(seq_len(n)))
plot(0:50, sizes, xlab = "Length", ylab = "Size (bytes)", 
  type = "s")
```

La première conclusion est que même un objet vide occupe 40 bytes de mémoire (soit $40 \times 8 = 320$ bits). La seconde est que le besoin mémoire de `R` n'est pas tout à fait linéaire: la progression est en escalier. C'est parce que demander au système de la mémoire (appel caché à la fonction `malloc()`) est une opération coûteuse. Demander la mémoire à la volée ralentirait considérablement l'exécution de `R`. A la place, `R` demande de la mémoire par blocs et organise ceux-ci tant qu'ils ne sont pas pleins. Passé une taille critique (128 bytes), `R` arrêt de demander des emplacements en puissance de 2 (8, 16, 32, 48, 64, 128) pour être plus flexible. 

**`R` gère lui-même la mémoire que le système lui alloue.** Il le fait de manière à être le plus parcimonieux possible. Cela se retrouve dans le comportement de copie à l'identique. Un même emplacement peut être partagé par plusieurs objets, ce qui limite l'inflation en mémoire:

```{r}
x <- 1:1e6
pryr::object_size(x)

y <- list(x, x, x)
pryr::object_size(y)
```

<!---CommOM: tu pourrais parler de *deep copy* versus *shallow copy*, non?---->

`y` ne pèse pas trois fois plus que `x`. `R` fait pointer `y` vers `x` plutôt que le copier. Cet usage particulier de la mémoire est, comme nous le verrons, une des raisons de l'efficience des assignations par référence et donc du package `data.table`. 

Le même comportement s'observe avec les variables de type `character` car `R` a une gestion globale de celles-ci. Cela signifie que chaque `string` unique est stockée uniquement a un endroit et permet ainsi aux vecteurs de texte, qui ont potentiellement des valeurs redondantes, de consommer moins de mémoire:

```{r}
pryr::object_size("banana")

pryr::object_size(rep("banana", 10))
```


### Modification d'objets

Deux manières de modifier un objet sont possibles en `R`:

* **In-place modification**: `R` modifie `x` en place. Cela signifie que le nouvel objet prend la place mémoire de l'ancien. C'est une approche économe en mémoire. Les modifications de `data.table` sur lesquels nous reviendrons sont beaucoup plus efficaces et économes en mémoire que celles sur un `tibble` ou un `data.frame` parce que ce package est construit sur la modification par référence qui étend la modification en place.
* **Copy-on-modify**: `R` effectue une copie temporaire de `x` vers une nouvelle adresse, modifie la copie et ensuite pointe le nom de `x` vers le nouvel emplacement.

Selon les circonstances, `R` va faire de la modification en place ou en copie. Les fonctions de remplacement primitives (`[[<-, [<-, @<-, $<-, attr<-, attributes<-, class<-, dim<-, dimnames<-, names<-, and levels<-`) modifient en place. Ce n'est pas le cas des autres fonctions d'assignation: `[<-.data.frame` n'étant pas une fonction primitive, elle effectuera de la modification en copie. 

**Exercice**

Pour comprendre la différence entre ces deux approches, on va utiliser les fonctions `pryr::address` (qui donne le pointeur vers l'emplacement mémoire) et `tracemem` qui suit les changements d'adresse d'un objet.

1. Créez un vecteur `x <- 1:10` et une copie `y` de celui-ci. 
2. Utilisez `pryr::address` et `pryr::object_size`: quel enseignement en tire-t-on ?
3. Modifiez un élément du vecteur tout en utilisant `pryr::mem_used`. La modification sera `x[5] <- 6L`. Refaites appel à la fonction `pryr::address` et à la fonction `pryr::object_size`: quelle conclusion en tirez-vous sur la place de l'objet `y` en mémoire ? A-t-on fait de la modification en place ou en copie ?
4. Faites un appel `tracemem(x)` et modifiez deux fois d'affilée une valeur du vecteur `x`: qu'observez-vous ? 

<div class="fold s">
```{r, results = "hide"}
# Question 1
x <- 1:10
y <- x

# Question 2
c(pryr::address(x), pryr::address(y))
# [1] "0xfa05510" "0xfa05510"
pryr::object_size(x,y)
# 96 B

# Question 3
pryr::mem_change(x[5] <- 6L)
# -6.52 kB
c(pryr::address(x), pryr::address(y))
# "0x1d8380f0" "0xfa05510" 
pryr::object_size(x,y)
# 192 B

# Question 4
tracemem(x)
x[5] <- 6L
# tracemem[0x000000001d8380f0 -> 0x000000001d721648]:
x[5] <- 6L
# tracemem[0x000000001d721648 -> 0x000000001d6ef600]:
```
</div>

Comme @wickham2014advanced l'évoque, il est compliqué d'éviter les modifications par copie. S'il est nécessaire de les éviter pour des raisons de vitesse ou de mémoire, l'utilisation `Rcpp` ou `data.table` peut s'imposer.

Nous avons déjà évoqué un facteur à l'origine de la lenteur des boucles `for`: les appels répétés aux fonctions. Néanmoins, la raison principale de la lenteur des boucles provient généralement du comportement de copie de `R`. Lorsqu'on effectue une boucle sur un `data.frame`, chaque itération de la boucle copie cet objet. 


**Exercice**

1. Créer un dataframe `x <- data.frame(matrix(runif(100 * 1e4), ncol = 5))` et un vecteur stockant la médiane par colonne `medians <- vapply(x, median, numeric(1))`
2. Utiliser la fonction 
3. Faire une boucle for de 1 à 5:
    + Prend la colonne `i` et retranche à chaque élément du vecteur la médiane.
    + Réassigner le résultat à la colonne
Observez l'adresse de `x`

<div class="fold s">
```{r, results="hide"}
x <- data.frame(matrix(runif(100 * 1e4), ncol = 5))
medians <- vapply(x, median, numeric(1))

tracemem(x)
# "<00000000090AA718>"
for(i in 1:5) {
  x[, i] <- x[, i] - medians[i]
}
# tracemem[0x00000000090aa718 -> 0x0000000014f73ae0]: 
# tracemem[0x0000000014f73ae0 -> 0x0000000014f73990]: [<-.data.frame [<- 
# tracemem[0x0000000014f73990 -> 0x0000000014f73920]: [<-.data.frame [<- 
# tracemem[0x0000000014f73920 -> 0x0000000014f738b0]: 
# tracemem[0x0000000014f738b0 -> 0x0000000014f73760]: [<-.data.frame [<- 
# tracemem[0x0000000014f73760 -> 0x0000000014f736f0]: [<-.data.frame [<- 
# tracemem[0x0000000014f736f0 -> 0x0000000014f73680]: 
# tracemem[0x0000000014f73680 -> 0x0000000014f73530]: [<-.data.frame [<- 
# tracemem[0x0000000014f73530 -> 0x0000000014f734c0]: [<-.data.frame [<- 
# tracemem[0x0000000014f734c0 -> 0x0000000014f73450]: 
# tracemem[0x0000000014f73450 -> 0x0000000014f73300]: [<-.data.frame [<- 
# tracemem[0x0000000014f73300 -> 0x0000000014f73290]: [<-.data.frame [<- 
# tracemem[0x0000000014f73290 -> 0x0000000014f73220]: 
# tracemem[0x0000000014f73220 -> 0x0000000014f730d0]: [<-.data.frame [<- 
# tracemem[0x0000000014f730d0 -> 0x0000000014f73060]: [<-.data.frame [<-
```
</div>

Comme vous pouvez le voir, le `data.frame` est copié à chaque itération, et cela plusieurs fois. C'est parce que la fonction `[<-.data.frame` n'est pas une fonction primitive. L'utilisation des listes permet d'être beaucoup plus efficace.

**Exercice**

1. Créer `y` une copie de `x` sous forme de liste et faire `tracemem(y)`. Vous pouvez garder le vecteur précédent `medians`
2. Refaire la même chose que précédemment avec `y`
3. Observer et conclure

<div class="fold s">
```{r, results="hide"}
y <- as.list(x)

tracemem(y)
# "<0000000014FEAB30>"
for(i in 1:5) {
  y[[i]] <- y[[i]] - medians[i]
}
# tracemem[0x0000000014feab30 -> 0x00000000122a3600]:
```
</div>

### Pré-allocation de la mémoire

En raison du comportement de copie, il faut faire attention à l'allocation de mémoire lorsqu'on désire ajouter des éléments à un objet. Comme nous allons le voir, il est préférable de pré-allouer la taille d'un objet plutôt que de le faire croître ou alors utiliser une liste.

**Exercice**

On va programmer des fonctions dépendant d'un paramètre `n`.  

* **Méthode 1**:
    + Créer un vecteur `vec` vide de taille `n` (vous pouvez utiliser `NULL` ou `c()`)
    + Faire une boucle `for` de 1 à n ajoutant pour concatener l'élément `i` à `vec`

<div class="fold s">
```{r}
method1 = function(n) {
  vec = NULL # Or vec = c()
  for(i in seq_len(n))
    vec = c(vec, i)
  return(vec)
}
```
</div>

* **Méthode 2**:
    + Créer un vecteur de taille `n` en utilisant la fonction `numeric`
    + Assigner l'élément `i` dans `x[i]`

<div class="fold s">
```{r}
method2 = function(n) {
  vec = numeric(n)
  for(i in seq_len(n))
    vec[i] = i
  vec
}
```
</div>

* **Method 3**: créer une suite avec la fonction `seq_len`

<div class="fold s">
```{r}
method3 = function(n) seq_len(n)
```
</div>

* Comparer la performance des trois méthodes avec `microbenchmark::microbenchmark` avec $n=1000$: regarder les temps d'exécution en nanosecondes sur 100 répétitions des trois méthodes puis faire le graphique avec `ggplot2::autoplot`.

<div class="fold s">
```{r}
n <- 1e4
micro <- microbenchmark::microbenchmark(times = 100, 
                                        method1(n), method2(n), method3(n))
print(micro)
ggplot2::autoplot(micro)
```
</div>

* Comparer le poids final du vecteur de sortie de `method1` avec `n=1e5` et le besoin mémoire pour le générer avec `profvis::profvis` (cliquez sur l'onglet `data`)

<div class="fold s">
```{r, results="hide"}
n <- 1e5
profvis::profvis(x <- method1(n))
pryr::object_size(x)
```
</div>

La méthode optimale est bien-sûr celle utilisant la fonction adaptée. La méthode 2, loin d'être efficiente, est 200 fois plus rapide que la méthode 1. @gillespie2016efficient propose plus de résultats en faisant varier la taille du vecteur. Quand $n=10^7$, la méthode 1 prend une heure quand la méthode 2 fonctionne en 2 secondes et la méthode 3 est instantanée. En termes de consommation de RAM, le *profiling* nous montre que la méthode 1 est très gourmande: pour générer un vecteur d'une taille finale de 400kB, il faut environ 1.3 Gigas de RAM qu'on rend ensuite au système. 


La pré-allocation est une manière de réduire les besoins mémoires et donc d'accélérer le code. Une autre solution est l'utilisation de listes, qui peut être pratique lorsqu'on ne connaît pas la taille d'un élément à l'avance. Les listes, contrairement aux vecteurs ou matrices, stockent leurs éléments dans des emplacements mémoires non contigües ce qui permet d'ajouter un élément sans copier le reste de l'objet et permet ainsi la modification par passage.

<!----
CommOM: tu dis qu'utiliser des listes c'est bien. Je pense qu'il faut faire un paragraphe spécifique là-dessus, où tu donnes tous les avantages, et un ou deux exemples.
------>



#### Utilisation, libération et... saturation de la mémoire

La consommation de RAM peut connaître des variations marquées en très peu de temps. Sur un poste personnel, cela peut amener le système d'exploitation à mobiliser toute la RAM disponible au détriment des autres logiciels. Sur un serveur commun, cela peut amener un utilisateur, sans qu'il s'en rendre compte, à mobiliser une part importante des ressources collectives. Lorsqu'on fait des traitements importants, il devient souvent nécessaire d'avoir un suivi fin de la consommation de mémoire vive. 

**La fonction `pryr::mem_used()` permet de connaître la mémoire consommée par l'ensemble des objets `R`.** 

```{r}
pryr::mem_used()
```

Cette fonction ne mesure toutefois pas la consommation totale de mémoire de `R` car elle ne prend en compte que les objets stockés actuellement en mémoire et pas l'ensemble de la mémoire qui a été allouée par `R`. Par exemple, si un objet volumineux a été supprimé, `R` ne rend pas nécessairement au système d'exploitation la mémoire ainsi libérée. De même, ce chiffre n'incorpore pas le poids mémoire de l'interpréteur `R` (ni de l'IDE utilisé, par exemple `Rstudio`). Pour connaître le poids mémoire effectif, il faut utiliser `htop` (Linux) ou le gestionnaire de tâches de Windows.   

Il n'est pas toujours évident, du fait de l'utilisation complexe de la mémoire par `R`, de suivre les besoins en mémoire vive d'une session. Par exemple, la création de l'objet `y` ci-dessous ne double pas les besoins mémoire mais nécessite tout de même plus de ressources. Supprimer l'objet •`x` libère plus de mémoire que la création du vecteur `y` en a demandé... 

```{r}
# Create a big object
pryr::mem_change(x <- 1:1e7)

# Also point to 1:1e6 from y
pryr::mem_change(y <- x)

# Remove x, no memory freed because y is still pointing to it
pryr::mem_change(rm(x))

# Now nothing points to it and the memory can be freed
pryr::mem_change(rm(y))

```


Dans certains langages, il est nécessaire d'explicitement supprimer des objets pour que leur espace mémoire soit libéré. `R` adopte une approche différente, celle du **garbage collection** (GC). GC libère automatiquement la mémoire occupée par un objet qui n'existe plus (en inspectant les espaces qui pointent vers des noms d'objets n'existant pas). Même si `R` effectue fréquemment des GC, **il est nécessaire d'appeler la commande `gc()` de temps en temps (en particulier après l'appel d'une fonction ayant effectué un gros traitement) pour rendre de la RAM au système d'exploitation.** Sur un serveur partagé, cela permet de réduire les risques de saturation à cause de sessions inactives qui n'ont plus besoin de la mémoire qu'elles mobilisent^[@wickham2014advanced dans son ouvrage développe l'idée que la commande `gc()` est inutile car la GC étant automatique, évoquer `gc()` ne libère aucun espace mémoire mais prend du temps. En pratique, je trouve que c'est faux (`gc()` permet souvent de libérer plusieurs gigaoctets) et que sur un serveur partagé, c'est une règle de bonne conduite pour permettre à chacun d'avoir des chances de disposer de mémoire quand il en a besoin.]. 

Outre la lenteur d'exécution, **le principal risque d'une mauvaise gestion de la mémoire est d'aboutir à une saturation de la RAM**. Les problèmes de saturation de la RAM se traduisent généralement par un message du type `cannot allocate a vector of size ** Mb` (quand la session ne *crashe* pas...). 


## Conclusion

**TO DO**


#### Aparté: comparaison des boucles avec Python et C++


De nombreux tests de performance sont disponibles en ligne, notamment sur cette [page](https://h2oai.github.io/db-benchmark/). On propose un test plus modeste de comparaison des performances de `Python`, `R` et `C++`. Pour exécuter du code `Python` depuis `R`, on utilise le package `Reticulate`^[Pour les personnes utilisant l'intégration continue, cela nécessite d'adapter le dépôt `rocker` généralement utilisé. Un exemple que je propose est le dépôt `pocker`, disponible sur [dockerhub](https://cloud.docker.com/u/linogaliana/repository/docker/linogaliana/pocker) [github](https://github.com/linogaliana/pocker), [gitlab](https://gitlab.com/linogaliana/pocker). Pour une explication de la raison d'être d'une image `docker` de ce type, ce [post](https://linogaliana.netlify.com/post/pocker/pocker-a-docker-container-to-use-r-and-python-together/)].


```{r}
library(reticulate)
```

On se propose d'étudier la vitesse de ces langages sur l'opération très simple déjà évoquée `x <- x+1`. En `Python`, cela peut s'écrire:


```{python, eval = TRUE}

#conda_create("r-reticulate")
#reticulate::conda_install("r-reticulate", "scipy")

import numpy as np
import time
import statistics

# VERSION BOUCLE
def test(n = 1000): 
  start = time.time()
  x = np.random.uniform(0,1,n)
  for i in range(n):
    x[i] = x[i] + 1
  end = time.time()
  return((end - start)*100000000)  # Nanoseconds

exec_time_python = [test(100000) for k in range(100)]

# VERSION VECTORISEE
def test2(n = 1000):
  start = time.time()
  x = np.random.uniform(0,1,n) + 1
  end = time.time()
  return((end - start)*100000000)  # Nanoseconds

exec_time_python2 = [test2(100000) for k in range(100)]

```


A noter que l'utilisation de `reticulate` peut induire un surcoût par rapport à une utilisation de `Python` dans un environnement dédié (`Pycharm`, `jupyter`...). La solution équivalente en `R ` est


```{r}
# VERSION BOUCLE
test <- function(n = 1000){
  start <- Sys.time()
  x <- runif(n)
  for (i in seq_len(n)){x[i] <- x[i] + 1}
  end = Sys.time()
  return((end - start)*100000000) # Nanoseconds
}

# VERSION VECTORISEE
test2 <- function(n = 1000){
  start <- Sys.time()
  x <- runif(n) + 1
  end = Sys.time()
  return((end - start)*100000000)  # Nanoseconds
}

exec_time_R <- replicate(100, test(100000))
# median(exec_time_R*1000)
exec_time_R2 <- replicate(100, test2(100000))
```


La traduction `C++` est presque immédiate grâce à `Rcpp` (cf. chapitre dédié):


```{Rcpp, eval = TRUE}
#include <Rcpp.h>
using namespace Rcpp;
#include <Rcpp/Benchmark/Timer.h>

// [[Rcpp::export]]
int test_cpp(int n) {
  
    // start the timer
  Timer timer;
  timer.step("start");        // record the starting point
  
  NumericVector x = runif(n);
  
  for (int i=0; i < n; ++i) {
    x[i] += 1;
  }

  timer.step("end");   // record the final step    
  
  
  NumericVector res(timer);   // 
  for (int i=0; i<res.size(); i++) {
    res[i] = res[i] / n;
  }
  return (res[1]-res[0]); // Nanoseconds    
  
  //return x;
}
```


```{r, eval = TRUE}
exec_time_cpp <- replicate(100, test_cpp(100000))
```

On peut reproduire un graphique proche de celui produit par `microbenchmark` pour déterminer la vitesse d'exécution des différentes approches:

```{r}

exec_times <- do.call(rbind,list(
  data.frame(t = py$exec_time_python, langage = "python [loop]"),
  data.frame(t = exec_time_R, langage = "R [loop]"),
  data.frame(t = py$exec_time_python2, langage = "python"),
  data.frame(t = exec_time_R2, langage = "R [base]"),
  data.frame(t = exec_time_cpp, langage = "Rcpp")
))
ggplot2::ggplot(data = exec_times) + ggplot2::geom_violin(ggplot2::aes(x = langage, y = log(t))) +
  ggplot2::labs(y = "Iteration time in (log) nanoseconds")
```






### References



<!------------------------

# Example: Monte-Carlo integration

It’s also important to make full use of R functions that use vectors. For example, suppose we wish to estimate the integral 

$$
\int_0^1 x^2dx
$$

using a Monte-Carlo method. Essentially, we throw darts at the curve and count the number of darts that fall below the curve

Monte Carlo Integration

```{r, eval = FALSE}

Initialise: hits = 0
for i in 1:N:
  Generate two random numbers, U1,U2, between 0 and 1
  If U2<U21,
    then hits = hits + 1
end for

Area estimate = hits/N
```

Implementing this Monte-Carlo algorithm in `R` would typically lead to something like:

```{r}
monte_carlo = function(N) {
  hits = 0
  for (i in seq_len(N)) {
    u1 = runif(1)
    u2 = runif(1)
    if (u1 ^ 2 > u2)
      hits = hits + 1
  }
  return(hits / N)
}
```

In R this takes a few seconds:

```{r}
N = 500000
system.time(monte_carlo(N))
```

In contrast a more R-centric approach would be

```{r}
monte_carlo_vec = function(N) sum(runif(N)^2 > runif(N))/N
```

The monte_carlo_vec() function contains (at least) four aspects of vectorisation

1. The runif() function call is now fully vectorised;
1. We raise entire vectors to a power via ^;
1. Comparisons using > are vectorised;
1. Using sum() is quicker than an equivalent for loop.

The function `monte_carlo_vec()` is around 30 times faster than monte_carlo():
```{r}
N = 500000
system.time(monte_carlo_vec(N))
```
----------->
