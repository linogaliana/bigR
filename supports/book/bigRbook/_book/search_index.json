[
["index.html", "A Minimal Book Example Chapter 1 Prerequisites", " A Minimal Book Example Yihui Xie 2019-09-24 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2019) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["literature.html", "Chapter 3 Literature", " Chapter 3 Literature Here is a review of existing methods. "],
["data-table-un-package-efficace-en-r.html", "Chapter 4 data.table: un package efficace en R 4.1 Principe 4.2 Lecture et écriture de données 4.3 La sémantique data.table 4.4 Requêtes sur des lignes", " Chapter 4 data.table: un package efficace en R R est réputé être un langage lent et de nombreux exemples sur internet proposent des comparatifs de vitesse qui suggèrent que python est plus rapide (nous avons déjà proposé un petit comparatif dans l’introduction de ce cours). Il est vrai que le langage de base R est lent mais, pour être honnête, avec le développement de data.table et tidyverse, la grammaire de base est de moins en moins utilisée de manière exclusive. Pour rendre réellement compte des performances du langage R, il convient donc de mesurer les vitesses d’exécution de ces deux approches. A ce jeu, data.table est très performant: ——–&gt; Dans ce chapitre, nous allons exlorer la manière dont data.table permet de résoudre efficacement de nombreuses tâches de maniement de données. Nous comparerons la vitesse et les besoins en mémoire par rapport à une approche tidyverse. Pour approfondir certains aspects du package (notamment l’indexation secondaire que nous laisserons de côté), plusieurs vignettes sont disponibles sur la page de documentation du package1 et un résumé des commandes standards est disponible ici. Sur la grammaire La syntaxe data.table a mauvaise presse auprès des utilisateurs de dplyr (ce post stack overflow est un bon exemple de la guerre larvée entre les utilisateurs des deux approches.2). Il y a un coût d’apprentissage à la syntaxe data.table lorsqu’on est un habitué à la grammaire du tidyverse. Cependant, comme ces deux approches reposent sur la philosophie du split-apply-combine, passé la forme différente, ce qu’on sait faire avec l’un peut facilement être traduit avec l’autre. Je recommande vivement la consultation de ce post qui compare de manière extensive les deux grammaires. Les 5 verbes fondamentaux du tidyverse (filter, select, mutate, arrange et summarise) ont une traduction en data.table. Comme Hadley Wickham (créateur de dplyr) l’évoque, il est vrai que sans connaissance de data.table, les verbes équivalents sont moins intelligibles que ceux du tidyverse. En revanche, une fois habitué, cette syntaxe est très puissante et permet de réaliser des opérations très complexes de manière assez aisée (peut-être plus qu’en utilisant les nested dataframes du tidyverse)3. Pourquoi faire du data.table quand on sait faire du tidyverse? La citation ci-dessous, issue du post déjà évoqué, souligne bien que l’objectif de ce cours, qui est de traiter efficacement de grosses bases de données en R, ne correspond pas à un cas d’usage autour duquel le tidyverse est construit: Memory and performance […] to me, they’re not that important. Most R users work with well under 1 million rows of data, and dplyr is sufficiently fast enough for that size of data that you’re not aware of processing time. We optimise dplyr for expressiveness on medium data; feel free to use data.table for raw speed on bigger data. Hadley Wickham (lien) data.table est plus rapide mais est aussi moins gourmand en RAM. Il s’agit donc d’une solution intéressante pour des bases de données de grosses tailles (comme les données administratives) mais qui rentrent encore en mémoire. Nous verrons dans le dernier chapitre des solutions out-of-memory pour les bases qui ne peuvent être lues en R parce que trop volumineuses (ou pour lesquelles des traitements statistiques, par exemple une régression linéaire, amènerait à saturer la RAM) data.table en production data.table ne dépend que de R base et n’importe de fonctions que du package methods (présent dans la distribution de base de R). Cela signifie que le package est extrêmement robuste puisque la distribution R base est très stable. Le tidyverse étant un écosystème très récent, il est beaucoup moins stable que data.table. Pour des projets en production, on peut donc se fier à data.table ; à condition de faire attention aux effets induits du comportement de passage par référence sur lequel on reviendra. L’utilisation de dplyr en production peut poser problème. En effet, il ne s’agit pas d’un langage de programmation fonctionnelle mais d’une syntaxe visant à faciliter le maniement de données. dplyr étant un package jeune, il n’est pas aussi stable que data.table: certaines fonctionnalités peuvent rapidement ne plus être maintenues au profit d’autres. 4.1 Principe data.table est un package qui propose une version améliorée des objets de base que sont les data.frames. Cependant, contrairement aux dataframes, on peut faire beaucoup plus que sélectionner des lignes ou des colonnes dans les crochets df[...]. En fait, on peut penser les instructions à l’intérieur du crochet comme des requêtes SQL mises en forme différemment. L’une des forces du package, sur lequel nous reviendrons, est que de nombreuses fonctionnalités de celui-ci reposent sur la modification par référence: pour créer une colonne y = x+1 on n’a pas besoin de perdre du temps et mobiliser de la mémoire pour effectuer une copie temporaire du dataframe. La syntaxe data.table prend la forme générale Structure de la syntaxe data.table On peut convertir un objet en data.table en utilisant data.table::setDT ou data.table::as.data.table. Pour créer un data.table, on utilisera la fonction data.table::data.table comme on utiliserait la fonction data.frame pour créer un dataframe. Comme avec un tibble, la visualisation est améliorée par rapport à un data.frame standard: dt &lt;- data.table::data.table(x = runif(1e4), y = runif(1e3)) dt ## x y ## 1: 0.62092928 0.1174102 ## 2: 0.02090176 0.7318420 ## 3: 0.34823912 0.6709725 ## 4: 0.76787509 0.1267709 ## 5: 0.53168010 0.4541203 ## --- ## 9996: 0.13959255 0.2804789 ## 9997: 0.47707891 0.2284737 ## 9998: 0.34601047 0.1457653 ## 9999: 0.05658275 0.6088700 ## 10000: 0.35669310 0.8217911 Les numéros de lignes sont écrits avec un :. Contrairement à un data.frame, les colonnes de type character ne sont jamais converties en factor par défaut. Un data.table n’utilise pas de rownames: il y a un outil beaucoup plus puissant, les clés sur lesquelles nous reviendrons. Par défaut, les classes des colonnes ne sont pas affichées. Si vous désirez les afficher, comme cela est fait sur les tibble, vous pouvez changer l’option associée de la manière suivante: options(datatable.print.class = TRUE) data.table::as.data.table(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;fctr&gt; ## 1: 5.1 3.5 1.4 0.2 setosa ## 2: 4.9 3.0 1.4 0.2 setosa ## 3: 4.7 3.2 1.3 0.2 setosa ## 4: 4.6 3.1 1.5 0.2 setosa ## 5: 5.0 3.6 1.4 0.2 setosa ## --- ## 146: 6.7 3.0 5.2 2.3 virginica ## 147: 6.3 2.5 5.0 1.9 virginica ## 148: 6.5 3.0 5.2 2.0 virginica ## 149: 6.2 3.4 5.4 2.3 virginica ## 150: 5.9 3.0 5.1 1.8 virginica options(datatable.print.class = FALSE) Les data.table héritant de caractéristiques des data.frame, on peut normalement leur appliquer toutes les méthodes qui s’appliquent aux data.frames de base. En particulier, on peut appliquer les fonctions de maniement des données du tidyverse aux data.tables: on peut utiliser stringr pour le maniement de caractères, lubridate pour des colonnes temporelles, etc. Avant d’appliquer une fonction du tidyverse à un data.table, il peut être utile de vérifier qu’il n’existe pas un équivalent déjà proposé par le package data.table. Par exemple, plutôt que d’utiliser la fonction stringr::str_split_fixed pour séparer une colonne en fonction d’un caractère, on utilisera data.table::tstrsplit: dt &lt;- data.table::data.table(x = paste0(&quot;x_&quot;,runif(1e4))) df &lt;- dplyr::as_tibble(dt) m &lt;- microbenchmark::microbenchmark( dt[,data.table::tstrsplit(dt$x, &quot;_&quot;, fixed = TRUE)], stringr::str_split_fixed(df$x,&quot;_&quot;, n = 2) ) ggplot2::autoplot(m) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. m ## Unit: milliseconds ## expr min lq ## dt[, data.table::tstrsplit(dt$x, &quot;_&quot;, fixed = TRUE)] 2.968391 3.245703 ## stringr::str_split_fixed(df$x, &quot;_&quot;, n = 2) 5.790330 6.400540 ## mean median uq max neval cld ## 4.558555 4.176299 5.399796 10.34383 100 a ## 8.443003 7.360672 8.988447 25.87578 100 b Sur ce problème simple, la solution data.table est 2 fois plus rapide. On proposera, tout au long de ce chapitre, des alternatives data.table aux 5 verbes fondamentaux de la grammaire tidyverse (filter, select, mutate, arrange et summarise). Il existe un package dtplyr qui propose des fonctions identiques à celles de dplyr (filter, mutate…) sur des objets data.table. Comme il s’agit d’une surcouche (qui traduit une fonction dplyr en l’équivalent data.table implicitement) il y a une perte (limitée) de performance inhérente à l’utilisation de ce package. Ce package a pour principalement inconvénient de ne pas proposer la copie par référence permise par l’opérateur := (sur lequel nous reviendrons) dans le verbe équivalent mutate ; une des forces du package data.table disparaît. 4.1.1 Aparté: sur l’évaluation standard Il existe deux manières de faire référence à un objet en R: L’évaluation standard: L’évaluation non-standard: les noms font référence Par exemple, Approche Evaluation standard (SE) Evaluation non standard (NSE) dplyr df %&gt;% dplyr::filter(x &lt; 10) df %&gt;% dplyr::filter(!!rlang::sym(\"x\")&lt;10) dplyr df[x&lt;10] df[get('x')&lt;10] En évaluation non standard, l’objet x est compris comme faisant partie de l’environnement en question (par exemple le dataframe df ou le data.table dt). Ce n’est que s’il n’existe pas dans cet environnement que R cherchera dans l’environnement global data.table et dplyr reposent tous deux sur l’ingration standard (on cite les noms de colonnes sous la forme dt[,x] ou df %&gt;% mutate(x) ce qui relève de l’intégration standard). L’intégration de l’évaluation standard, qui est à la fois pratique dans une approche de programmation fonctionnelle (on peut passer des noms de variable en argument d’une fonction) et désirable pour la reproductibilité des programmes est ardue à mettre en oeuvre dans dplyr alors qu’elle est très simple en data.table4. 4.2 Lecture et écriture de données Pour montrer l’intérêt des fonctions d’import de data.table nous allons utiliser la base de données sirus 2017 (extrait de Sirene) qui avait servi au hackathon de l’INSEE en 2018. Le fichier fait 2GB sur disque et environ 3.5 GB en RAM une fois importé entièrement dans R. 4.2.1 Comparaison des fonctions de lecture Dans ce premier comparatif, on va comparer les vitesses d’exécution des fonctions: data.table::fread readr::read_csv read.csv du langage de base On utilise le fichier sirus_abstract.csv qui est un extrait de sirus sur un million d’observations. Mon ordinateur a 4 coeurs et 8 Go de RAM. # Code ayant généré sirus_abstract.csv df &lt;- data.table::fread(&quot;./data/sirus_2017.csv&quot;) data.table::fwrite( df[sample(.N, size = 1e6, replace = FALSE)], file = &quot;./data/sirus_abstract.csv&quot; ) La fonction de lecture du package data.table est fread pour fast read. Elle permet de lire des fichiers texte, qu’ils soient zippés ou non. Pour accélérer la routine, la fonction repose sur le principe du multithreading (cf. chapitre précédent) dans C. Exercice Faire un profiling, avec la fonction profvis::profvis des méthodes suivantes (n’hésitez pas à consulter l’aide de chaque fonction pour obtenir le nom des arguments) read.csv en faisant attention à ne pas oublier l’option stringsAsFactors = FALSE et l’argument sep readr::read_csv en désactivant la barre de progrès data.table::fread en désactivant également la barre de progrès Je recommande d’appeler la commande gc() entre chaque commande pour mesurer de manière plus précise les besoins en mémoire spécifiques à chaque méthode. profvis::profvis({ df_1 &lt;- read.csv(&quot;./data/sirus_abstract.csv&quot;, sep = &quot;,&quot;, stringsAsFactors = FALSE) gc() # gc pour ne pas biaiser la mesure des besoins mémoires df_2 &lt;- readr::read_csv(&quot;./data/sirus_abstract.csv&quot;, progress = FALSE) gc() # gc pour ne pas biaiser la mesure des besoins mémoires df_3 &lt;- data.table::fread(&quot;./data/sirus_abstract.csv&quot;, showProgress = FALSE) }) ## Parsed with column specification: ## cols( ## .default = col_character(), ## sirus_id = col_integer(), ## nic = col_integer(), ## eff_3112_et = col_integer(), ## eff_etp_et = col_integer(), ## eff_et_effet_daaaammjj = col_integer(), ## adr_et_voie_num = col_integer(), ## adr_et_cedex = col_integer(), ## adr_et_post = col_integer(), ## nic_siege = col_integer(), ## unite_type = col_integer(), ## region = col_integer(), ## region_impl = col_integer(), ## cj = col_integer(), ## eff_3112_uniteLegale = col_integer(), ## eff_etp_uniteLegale = col_integer(), ## eff_effet_daaaammjj_uniteLegale = col_integer(), ## x = col_double(), ## y = col_double(), ## SourceXYW = col_double(), ## qual = col_double() ## ) ## See spec(...) for full column specifications. ## Warning in rbind(names(probs), probs_f): number of columns of result is not ## a multiple of vector length (arg 1) ## Warning: 7 parsing failures. ## row # A tibble: 5 x 5 col row col expected actual file expected &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; actual 1 30438 adr_et_voie_n~ no trailing charac~ -7 &#39;./data/sirus_abstract~ file 2 101216 adr_et_voie_n~ no trailing charac~ A &#39;./data/sirus_abstract~ row 3 110067 adr_et_voie_n~ no trailing charac~ &quot; 4&quot; &#39;./data/sirus_abstract~ col 4 113507 adr_et_voie_n~ no trailing charac~ &quot; 3&quot; &#39;./data/sirus_abstract~ expected 5 157030 adr_et_voie_n~ no trailing charac~ -6 &#39;./data/sirus_abstract~ ## ... ................. ... .......................................................................... ........ .......................................................................... ...... .......................................................................... .... .......................................................................... ... .......................................................................... ... .......................................................................... ........ .......................................................................... ## See problems(...) for more details. La fonction data.table est à la fois plus rapide: 3 secondes contre 6 pour readr et 18 pour la fonction de base (en console. Les résultats peuvent différer dans le rendu du markdown) moins gourmande en RAM: pour importer un jeu de données faisant environ 400MB en RAM, il est nécessaire d’utiliser 900 MB pour readr et 1.3GB pour la fonction de base. data.table est lui très parcimonieux puisqu’il ne nécessite pas plus que les 400MB nécessaires. La fonction de base est très lente. Pourtant, elle est codée en C comme la fonction de data.table. Le principal problème avec la fonction de base, que les fonctions readr et fread évitent est d’importer l’ensemble du fichier sous forme character et convertir a posteriori les colonnes pouvant l’être sous des formats numériques. Les fonctions read_csv et fread inspectent le fichier avant de l’importer (les premières lignes pour read_csv ; les premières et dernières lignes pour fread) On peut utiliser l’option verbose = TRUE pour avoir une meilleure idée de la manière dont fread fonctionne df_3 &lt;- data.table::fread(&quot;./data/sirus_abstract.csv&quot;, verbose = TRUE) 4.2.2 Les options de la fonction fread Jusqu’à présent, on importait le fichier sans considération de l’information qui nous intéressait. Il arrive souvent que l’on ne soit intéressé que par une partie d’un fichier (certaines colonnes, un échantillon de lignes). Pour cela, on peut utiliser quelques options de la fonction data.table::fread: nrows: on ne garde que N lignes select: on ne sélectionne que les colonnes qui nous intéressent drop: on abandonne des colonnes qui ne nous intéressent pas cmd: commande shell à exécuter lors de la lecture. Il s’agit d’une option très puissante que l’on va utiliser uniquement pour sélectionner un sous-échantillon des données à partir d’une expression régulière L’objectif est d’être moins gourmand en RAM en ne gardant que les observations (lignes ou colonnes) qui nous intéressent. Par exemple, on peut n’importer qu’une ligne nrows = 1 pour connaître le nom des colonnes cols &lt;- colnames( data.table::fread(&quot;./data/sirus_2017.csv&quot;, nrows = 1) ) On va utiliser ce vecteur cols par la suite donc gardez le dans l’environnement. Maintenant qu’on peut visualiser les colonnes, on peut importer les parties du fichier qui nous intéressent Exercice Utiliser l’option select pour garder les colonnes sirus_id, nic, ape, eff_3112_et Utiliser l’option drop et le vector cols pour éliminer les colonnes qui commencent par adr_ ou eff_. Vous pouvez utiliser, pour filtrer cols: une expression régulière et grepl ou la fonction de base startsWith. Si vous êtes à l’aise avec le package stringr ou stringi il y a aussi la possibilité de les utiliser # Question 1 --------------- profvis::profvis({ df &lt;- data.table::fread(&quot;./data/sirus_2017.csv&quot;, select = c(&quot;sirus_id&quot;, &quot;nic&quot;, &quot;ape&quot;, &quot;eff_3112_et&quot;)) }) # Question 2 --------------- cols_to_rm &lt;- cols[(startsWith(cols, &quot;adr_&quot;) | startsWith(cols, &quot;eff_&quot;))] # Methode base # cols_to_rm &lt;- cols[!grepl(&quot;^(adr_|eff_)&quot;, cols)] # Methode base avec regex profvis::profvis({ df &lt;- data.table::fread(&quot;./data/sirus_2017.csv&quot;, drop = cols_to_rm) }) Les options précédentes permettaient de sélectionner des colonnes. Pour filtrer des lignes, on peut utiliser une commande shell. C’est une fonctionnalité très puissante parce qu’elle permet D’éviter de charger des données qui mobilisent de la mémoire temporairement. Comme on l’a évoqué dans le chapitre 1, R a une gestion globale des données caractères: faire le filtre avant de charger les données évite d’ajouter des éléments au cache pour les supprimer ensuite Les commandes shell sont plus rapides que des surcouches R Exercice Je recommande de tester sur le fichier sirus_abstract.csv avant d’essayer sur sirus_2017.csv On ne va garder que des observations où apparaît la ville TOULOUSE. On ne va garder que les observations où n’apparaît pas la ville TOULOUSE git_bash &lt;- &quot;C:\\Users\\W3CRK9\\AppData\\Local\\Programs\\Git\\git-bash.exe&quot; # df &lt;- data.table::fread(cmd = &quot;findstr TOULOUSE ./data/sirus_abstract.csv * 2&gt;nul&quot;) # Question 1 df &lt;- data.table::fread(cmd = sprintf(&#39;&quot;%s&quot; --login grep TOULOUSE ./data/sirus_abstract.csv&#39;, git_bash)) # Question 2 df &lt;- data.table::fread(cmd = sprintf(&#39;start &quot;%s&quot; -c grep -v TOULOUSE ./data/sirus_abstract.csv&#39;, git_bash)) La commande cmd permet d’exécuter beaucoup plus que des commandes grep. Toute commande shell peut être utilisée. Le problème sur Windows provient du fait que la commande sous-jacente (system) fait appel à l’invite de commande DOS qui est beaucoup moins complète qu’un terminal unix5 4.2.3 Import d’autres formats de données Formats binaires fst: c’est un format beaucoup plus rapide à l’import que le csv. La fonction fst::read_fst propose une option as.data.table qu’on peut évaluer à TRUE (la valeur par défaut est FALSE ce qui importe sous forme de data.frame). Formats sas: pas de miracle, il faut utiliser la fonction haven::read_sas et convertir en data.table avec data.table::setDT function. La version de développement de haven intègre depuis Juillet 2019 des paramètres qui peuvent être précieux pour réduire les besoins mémoires et CPU lors de l’import: col_select et n_max. Ces fonctionnalités ne sont pas encore disponibles sur la version CRAN en revanche. 4.2.4 Comparaison des écritures Faire microbenchmark df &lt;- data.frame(x = runif(10e4), y = runif(10e4)) dt &lt;- data.table::setDT(df) dtib &lt;- dplyr::as_tibble(df) m &lt;- microbenchmark::microbenchmark( write.csv(df, file = &quot;base.csv&quot;), data.table::fwrite(dt, file = &quot;datatable.csv&quot;), readr::write_csv(dtib, path = &quot;readr.csv&quot;), times = 20 ) ggplot2::autoplot(m) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. file.remove(&quot;base.csv&quot;,&quot;datatable.csv&quot;,&quot;readr.csv&quot;) ## [1] TRUE TRUE TRUE Faire profvis df &lt;- data.frame(x = runif(10e6), y = runif(10e6)) dt &lt;- data.table::setDT(df) dtib &lt;- dplyr::as_tibble(df) profvis::profvis({ data.table::fwrite(dt, file = &quot;datatable.csv&quot;) readr::write_csv(dt, path = &quot;readr.csv&quot;) }) file.remove(&quot;datatable.csv&quot;,&quot;readr.csv&quot;) ## [1] TRUE TRUE 4.3 La sémantique data.table 4.3.1 Retour sur les copies On a évoqué, lors du premier chapitre, la différence entre la copie en place et la copie en modification. On distingue parfois de : deep copy (copie profonde): la copie est indépendante de l’objet copié. Autrement dit, les modifications faites sur la copie (on parle parfois d’objet enfant, child object) n’affectent pas l’objet copié (l’objet parent, parent object). Il y a duplication dans un autre espace mémoire de l’objet initial. shallow copy (copie superficielle): une relation de référence (typiquement via un pointeur) est créée entre l’objet copié et la copie. Les modifications faites sur les éléments communs affectent les deux objets: modifier les éléments partagés du parent ou de l’enfant modifiera l’autre objet. Il n’y a pas de duplication de l’espace physique en mémoire. Ce tutoriel explique très bien, à partir du langage python, la différence entre ces deux types de comportement. Les shallow copy réduisent les besoins mémoires et sont donc plus performantes. data.table repose sur ce principe: on modifie les colonnes ou attributs d’un data.table par référence, ce qui réduit grandement les besoins mémoire de R (en temps normal, R a besoin de 2-3x plus de mémoire que la taille de l’environnement. Avec data.table, on peut réduire drastiquement cette marge de manoeuvre). Les shallow copy nécessitent en revanche de prendre plus de précautions si on désire ré-utiliser un objet parent. df &lt;- data.frame(&quot;id&quot; = rep(c(&quot;a&quot;,&quot;b&quot;), 10), &quot;x&quot; = runif(20)) tracemem(df) ## [1] &quot;&lt;0000000029181E58&gt;&quot; y &lt;- seq_len(20) tracemem(y) ## [1] &quot;&lt;000000003ED1B270&gt;&quot; df$x &lt;- y # (1) -- replace entire column ## tracemem[0x0000000029181e58 -&gt; 0x00000000291744b8]: eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x00000000291744b8 -&gt; 0x00000000291744f8]: $&lt;-.data.frame $&lt;- eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x00000000291744f8 -&gt; 0x0000000029174578]: $&lt;-.data.frame $&lt;- eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; # or df$x[df$id == &quot;b&quot;] &lt;- y[1:10] # (2) -- subassign in column &#39;c&#39; ## tracemem[0x0000000029174578 -&gt; 0x000000002916e658]: eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x000000003ed1b270 -&gt; 0x0000000016b2f1f0]: eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x000000002916e658 -&gt; 0x000000002916e718]: eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x000000002916e718 -&gt; 0x000000002916e758]: $&lt;-.data.frame $&lt;- eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x000000002916e758 -&gt; 0x000000002916e798]: $&lt;-.data.frame $&lt;- eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; Jusqu’à R 3.1 (avril 2014), les copies profondes étaient automatiques. Cela amenait à copier plus d’une fois les objets. Une amélioration significative de performance a eu lieu avec la v3.1. L’assignation (1) ne provoque plus une copie profonde mais une copie superficielle. En revanche, même depuis 3.1+, l’opération (2) provoque toujours une copie profonde de l’ensemble de la colonne. Cela signifie que plus de colonnes sont modifiées dans la même assignation plus il y a de copies et donc de besoin mémoire 4.3.2 L’opérateur := L’opérateur data.table := est construit autour du principe du passage par référence qui évite donc la copie superficielle comme profonde. On peut le comprendre comme le mutate de dplyr sauf que l’opérateur data.table n’effectue pas de copie superficielle contrairement à la fonction dplyr (ce thread stack overflow peut être consulté sur le sujet). := utilise un pointeur d’où la parcimonie en termes de mémoire Pour créer une nouvelle colonne à un objet data.table, on n’a pas besoin d’utilser l’assignation &lt;-, il suffit d’appeler := de la manière suivante: # 1/ UNE SEULE COLONNE: ON CREE LA COLONNE x DT[,&#39;x&#39; := values] # EVALUATION STANDARD DT[,x := values] # EVALUATION NON STANDARD # 2/ PLUSIEURS COLONNES DT[, c(&quot;colA&quot;, &quot;colB&quot;, ...) := list(valA, valB)] DT[, `:=`(colA = valA, # valA is assigned to colA colB = valB # valB is assigned to colB) )] Malgré l’absence de &lt;-, le résultat est retourné de manière invisible. data.table et dplyr auront des performances proches lors de la création ex-nihilo d’une colonne. En revanche, data.table sera plus efficace lors de l’utilisation d’un objet déjà existant (au lieu de faire une copie superficielle, il utilisera directement un pointeur) Exercice Reprendre les données sirus complètes Créer la colonne myvar à partir de la commande runif en faisant attention de ne pas se tromper sur la longueur du vecteur aléatoire (utiliser la commande nrow ou la commande data.table équivalente .N) df &lt;- data.table::fread(&quot;./data/sirus_2017.csv&quot;) df[,&#39;myvar&#39; := runif(.N)] df[,`:=` (&#39;myvar1&#39; = myvar, &#39;myvar2&#39; = 2*get(&#39;myvar&#39;))] Comparer les besoins mémoire et la vitesse des approches data.table et dplyr. On va créer une variable cell_id qui alloue chaque observation à un carreau de 1km. On va mettre en pratique une astuce pratique, par exemple utilisée par le package btb. Pour cela, Créer df2 &lt;- dplyr::as_tibble(df). Créer des colonnes x_cell et y_cell qui sont, respectivement, égales à 1000round(x/1000) et 1000round(y/1000) Créer la colonne cell_id égale à paste(x_cell,y_cell, sep='_') Créer une colonne aléatoire w et creer w2 sous la forme w - mean(w) df2 &lt;- dplyr::as_tibble(df) import::from(&quot;magrittr&quot;,&quot;%&gt;%&quot;) profvis::profvis({ # Approche data.table # df[,c(&#39;x_cell&#39;,&#39;y_cell&#39;) := list( # 1000*round(x/1000), 1000*round(y/1000) # )] df[,`:=`( &#39;x_cell&#39; = 1000*round(x/1000), &#39;y_cell&#39; = 1000*round(y/1000) )] df[,&#39;cell_id&#39; := paste(x_cell,y_cell, sep=&#39;_&#39;)] # ou mieux en evaluation standard # df[,&#39;cell_id&#39; := paste(get(&#39;x_cell&#39;), # get(&#39;y_cell&#39;), sep=&#39;_&#39;)] # On verra par la suite qu&#39;on peut aussi faire # df[,c(&#39;x_cell&#39;,&#39;y_cell&#39;) := lapply(.SD, function(x) 1000*round(x/1000)), .SDcols = c(&#39;x&#39;,&#39;y&#39;)] df[,&#39;cell_id&#39; := paste(x_cell,y_cell, sep=&#39;_&#39;)] df[,&#39;w&#39; := runif(.N)] df[,&#39;w2&#39; := w - mean(w)] gc() # Approche dplyr df2 &lt;- df2 %&gt;% dplyr::mutate(&#39;x_cell&#39; = 1000*round(x/1000), &#39;y_cell&#39; = 1000*round(y/1000)) df2 &lt;- df2 %&gt;% dplyr::mutate(&#39;cell_id&#39; = paste(x_cell, y_cell, sep=&#39;_&#39;)) df2 &lt;- df2 %&gt;% dplyr::mutate(w = runif(nrow(df2))) df2 &lt;- df2 %&gt;% dplyr::mutate(w2 = w - mean(w)) }) ## Warning: The `printer` argument is deprecated as of rlang 0.3.0. ## This warning is displayed once per session. Pour supprimer des colonnes par référence, par exemple les colonnes précédemment créées myvar, myvar1 et myvar2, on les assignent à la valeur NULL df[, c(&quot;myvar&quot;,&quot;myvar1&quot;,&quot;myvar2&quot;) := NULL] Lorsqu’on désire passer un data.table à une fonction où sont effectués des changements par référence, on peut créer une copie de l’objet initial pour ne pas le modifier par référence (sans copie, même dans des environnements différents, l’objet parent peut être modifié). Dans ce cas, on utilise la fonction copy de la manière suivante newdf &lt;- data.table::copy(df) et ainsi on peut modifier newdf sans crainte. 4.3.3 La famille des fonctions set* Avec l’opérateur :=, les fonctions commençant par set permettent d’actualiser un data.table par référence: setnames: renommer des variables setorder et setorderv: trier les lignes en fonction d’une ou plusieurs colonnes setcolorder: changer l’ordre des colonnes setDT et setDF: transformer un df en dt ou vice-versa data.table dplyr setnames rename setorder arrange setcolorder select(..., everything()) Exercice Créer une variable aléatoire x dans le data.table df et la renommer u en utilisant les arguments old et new de data.table::setnames (vous pouvez retrouver dans l’aide d’autres manières de modifier les noms avec setnames) Trier par la variable region et adr_depcom Mettre les colonnes adr_et_l6 en première position 4.4 Requêtes sur des lignes Dans le contexte d’un data.table, i.e. au sein des crochets [], nous avons vu qu’on pouvait faire référence aux colonnes comme des variables. On l’a vu avec les colonnes mais c’est vrai également lorsqu’on fait une requête dans la première dimension, les lignes. Les requêtes sur la dimension i peuvent servir d’alternative à la fonction dplyr::filter (mais les requêtes sur la dimension i permettent de faire plus comme nous le verrons avec le remplacement conditionnel) df[region == 75] # NSE ## sirus_id nic ape apet eff_3112_et eff_etp_et ## 1: 7272529 46 6820B 6820B NA NA ## 2: 7371255 22 4789Z 4789Z NA NA ## 3: 7400765 17 8110Z 8110Z 0 0 ## 4: 7455306 14 8110Z 8110Z 1 1 ## 5: 7455371 18 8110Z 8110Z 0 0 ## --- ## 803069: 998774103 35 4939A 4939B 45 40 ## 803070: 998774103 68 4939A 4939B NA NA ## 803071: 998774103 76 4939A 4939B NA NA ## 803072: 998774103 92 4939A 4939B 43 41 ## 803073: 998774103 100 4939A 4939B NA NA ## eff_et_effet_daaaammjj enseigne_et1 nom_comm_et adr_et_loc_geo ## 1: NA 24256 ## 2: NA 1730000798 ## 3: 20151231 1601500215 ## 4: 20151231 1730001435 ## 5: 20151231 1730600737 ## --- ## 803069: 20151231 6448300107 ## 803070: NA 64495 ## 803071: NA 6410200500 ## 803072: 20151231 4031200104 ## 803073: NA 6416000224 ## adr_et_compl adr_et_voie_num adr_et_voie_repet ## 1: ## 2: COMMUNE DE RATTACHEMENT 1 ## 3: 9 ## 4: ## 5: ## --- ## 803069: ZI DE JALDAY 203 ## 803070: 2 ## 803071: 8 ## 803072: LOT N 28 ZI DE BOUCAU ## 803073: MAISON CARRICABURUA ## adr_et_voie_type adr_et_voie_lib adr_et_cedex adr_et_distsp ## 1: BD DE L HORIZON ## 2: RUE DE L&#39;HOTEL DE VILLE ## 3: RUE ABBE ROUSSELOT ## 4: RUE DES VOILIERS ## 5: AV DE PONTAILLAC ## --- ## 803069: RUE DES ARTISANS BP 133 ## 803070: LAC DE ST PEE ## 803071: RUE DES LISSES ## 803072: ZONE INDUSTRIELLE ## 803073: ALL DES MARRONNIERS ## sir_adr_et_com_lib adr_et_post adr_et_l1 ## 1: MARSAC SUR L&#39;ISLE 24430 MONSIEUR RENE BURGAUD ## 2: LA ROCHELLE 17000 MONSIEUR BERNARD LE RUYET ## 3: ANGOULEME 16000 COPROPRIETAIRES ## 4: LA ROCHELLE 17000 COPRO RES LES VOILIERS ## 5: ROYAN 17200 COPRO RES DE VERTHAMON ## --- ## 803069: SAINT JEAN DE LUZ 64500 LE BASQUE BONDISSANT ## 803070: SAINT PEE SUR NIVELLE 64310 LE BASQUE BONDISSANT ## 803071: BAYONNE 64100 LE BASQUE BONDISSANT ## 803072: TARNOS 40220 LE BASQUE BONDISSANT ## 803073: CAMBO LES BAINS 64250 LE BASQUE BONDISSANT ## adr_et_l2 adr_et_l3 adr_et_l4 ## 1: BD DE L HORIZON ## 2: COMMUNE DE RATTACHEMENT 1 RUE DE L&#39;HOTEL DE VILLE ## 3: 9 RUE ABBE ROUSSELOT ## 4: RUE DES VOILIERS ## 5: AV DE PONTAILLAC ## --- ## 803069: ZI DE JALDAY 203 RUE DES ARTISANS ## 803070: 2 LAC DE ST PEE ## 803071: 8 RUE DES LISSES ## 803072: LOT N 28 ZI DE BOUCAU ZONE INDUSTRIELLE ## 803073: MAISON CARRICABURUA ALL DES MARRONNIERS ## adr_et_l5 adr_et_l6 adr_et_l7 nic_siege ## 1: 24430 MARSAC SUR L&#39;ISLE NA 46 ## 2: 17000 LA ROCHELLE NA 22 ## 3: 16000 ANGOULEME NA 17 ## 4: 17000 LA ROCHELLE NA 14 ## 5: 17200 ROYAN NA 18 ## --- ## 803069: BP 133 64500 SAINT JEAN DE LUZ NA 35 ## 803070: 64310 SAINT PEE SUR NIVELLE NA 35 ## 803071: 64100 BAYONNE NA 35 ## 803072: 40220 TARNOS NA 35 ## 803073: 64250 CAMBO LES BAINS NA 35 ## unite_type region adr_depcom region_impl region_mult tr_eff_etp ## 1: 1 75 24256 75 MONO NN ## 2: 1 75 17300 75 MONO NN ## 3: 1 75 16015 75 MONO 00 ## 4: 1 75 17300 75 MONO 01 ## 5: 1 75 17306 75 MONO 00 ## --- ## 803069: 1 75 64483 75 QASI 21 ## 803070: 1 75 64483 75 QASI 21 ## 803071: 1 75 64483 75 QASI 21 ## 803072: 1 75 64483 75 QASI 21 ## 803073: 1 75 64483 75 QASI 21 ## cj denom denom_condense sigle enseigne ## 1: 1900 ## 2: 1200 ## 3: 9110 COPROPRIETAIRES COPROPRIETAIRES ## 4: 9110 COPRO RES LES VOILIERS COPRO RES LES VOILIERS ## 5: 9110 COPRO RES DE VERTHAMON COPRO RES DE VERTHAMON ## --- ## 803069: 5499 LE BASQUE BONDISSANT LE BASQUE BONDISSANT ## 803070: 5499 LE BASQUE BONDISSANT LE BASQUE BONDISSANT ## 803071: 5499 LE BASQUE BONDISSANT LE BASQUE BONDISSANT ## 803072: 5499 LE BASQUE BONDISSANT LE BASQUE BONDISSANT ## 803073: 5499 LE BASQUE BONDISSANT LE BASQUE BONDISSANT ## eff_3112_uniteLegale eff_etp_uniteLegale ## 1: NA NA ## 2: NA NA ## 3: 0 0 ## 4: 1 1 ## 5: 0 0 ## --- ## 803069: 88 81 ## 803070: 88 81 ## 803071: 88 81 ## 803072: 88 81 ## 803073: 88 81 ## eff_effet_daaaammjj_uniteLegale x y SourceXYW qual ## 1: NA 516517.0 6458140 NA 1.0 ## 2: NA 379742.2 6570605 NA 2.2 ## 3: 20151231 480041.8 6509406 NA 1.0 ## 4: 20151231 380250.0 6571150 430.1163 8.0 ## 5: 20151231 384650.0 6510950 1650.7574 8.0 ## --- ## 803069: 20151231 325373.2 6267049 NA 1.0 ## 803070: 20151231 326250.0 6262950 NA 9.0 ## 803071: 20151231 338340.1 6275927 NA 1.0 ## 803072: 20151231 339350.0 6281650 NA 9.0 ## 803073: 20151231 342850.0 6261250 430.1163 8.0 ## x_cell y_cell cell_id w w2 ## 1: 517000 6458000 517000_6458000 0.7780711 0.27808649 ## 2: 380000 6571000 380000_6571000 0.9288143 0.42882965 ## 3: 480000 6509000 480000_6509000 0.3000650 -0.19991960 ## 4: 380000 6571000 380000_6571000 0.5626787 0.06269411 ## 5: 385000 6511000 385000_6511000 0.9060610 0.40607636 ## --- ## 803069: 325000 6267000 325000_6267000 0.1723198 -0.32766478 ## 803070: 326000 6263000 326000_6263000 0.9629640 0.46297940 ## 803071: 338000 6276000 338000_6276000 0.9609991 0.46101445 ## 803072: 339000 6282000 339000_6282000 0.3693000 -0.13068461 ## 803073: 343000 6261000 343000_6261000 0.2302054 -0.26977926 # df[get(&#39;region&#39;) == 75] # SE Exercice Comparer la vitesse et le besoin mémoire de l’approche data.table et de dplyr::filter en ne sélectionnant les lignes situées dans les départements 75, 77, 78, 91, 92, 93, 94, 95 (variable region) apparaîssent les termes ECOLE, COLLEGE ou LYCEE (et leur pluriel) dans la variable denom (vous pouvez utiliser une expression régulière ou %in%) profvis::profvis({ df[(grepl(&quot;ECOLE(S?)|COLLEGE(S?)|LYCEE(S?)&quot;,denom)) &amp; (region %in% c(75, 77, 78, 91, 92, 93, 94, 95))] df %&gt;% dplyr::filter((grepl(&quot;ECOLE(S?)|COLLEGE(S?)|LYCEE(S?)&quot;,denom)) &amp; (region %in% c(75, 77, 78, 91, 92, 93, 94, 95))) }) Le package comporte quelques fonctions qui peuvent être pratiques pour effectuer des opérations de filtre: %in%: Extension du %in% de base %chin%: Equivalent mais pour des variables character %between%: Comme le nom l’indique, x %between% c(a,b) revient à x&gt;=a &amp; x&lt;=b %like%: identifier les variables qui matchent un pattern. Pour obtenir simplement un échantillon des données, on peut faire la commande suivante (en supposant qu’on désire 1000 lignes) df[sample(.N, size = 1e4, replace = FALSE)] ## sirus_id nic ape apet eff_3112_et eff_etp_et ## 1: 444245278 15 8010Z 8010Z NA NA ## 2: 302542014 14 6820B 6820B NA NA ## 3: 818292708 18 8690D 8690D NA NA ## 4: 789764024 20 7430Z 7430Z NA NA ## 5: 538301219 27 0145Z 0145Z NA NA ## --- ## 9996: 484645015 13 8299Z 8299Z NA NA ## 9997: 751971045 16 4791A 4791A NA NA ## 9998: 379723554 26 4399C 5630Z NA NA ## 9999: 411348857 11 0111Z 0111Z NA NA ## 10000: 804014777 10 8121Z 8121Z NA NA ## eff_et_effet_daaaammjj enseigne_et1 nom_comm_et adr_et_loc_geo ## 1: NA 53122 ## 2: NA 86246 ## 3: NA 43013 ## 4: NA 25572 ## 5: NA 07134 ## --- ## 9996: NA 51612 ## 9997: NA 24364 ## 9998: NA 5935000519 ## 9999: NA 51651 ## 10000: NA 9137700200 ## adr_et_compl adr_et_voie_num adr_et_voie_repet ## 1: ## 2: ## 3: ## 4: 12 ## 5: ## --- ## 9996: 24 B ## 9997: CHEMIN DE CHASSAGNE B ## 9998: 2 ## 9999: 8 ## 10000: 6 ## adr_et_voie_type adr_et_voie_lib adr_et_cedex adr_et_distsp ## 1: LE HAUT CRUE ## 2: LA GASSOTTE ## 3: LIEU DIT CHANTUZIER ## 4: RUE DES FONTAINES ## 5: L&#39;EXTERNAT ## --- ## 9996: RUE PAUL GOERG ## 9997: CHE CHASSAGNE ## 9998: PL DE L ARSENAL ## 9999: RUE DU PRESBYTERE ## 10000: SQ DE GRENOBLE ## sir_adr_et_com_lib adr_et_post adr_et_l1 ## 1: JUBLAINS 53160 MONSIEUR JEAN-CHRISTOPHE NICOLAU ## 2: SAINT SAVIN 86310 MONSIEUR JEAN BRIAUD ## 3: VISSAC AUTEYRAC 43300 MADAME SEVERINE PORTAL ## 4: TROUVANS 25680 MADAME SOPHIE SERVAIS ## 5: LAURAC EN VIVARAIS 7110 MONSIEUR JEREMY BERTRAND ## --- ## 9996: VERTUS 51130 GIE PHARMACIES DE CHAMPAGNE ## 9997: SAINT AMAND DE COLY 24290 MADAME LYNDA KLEIN-ESSINK ## 9998: LILLE 59800 MONSIEUR JEAN-PIERRE BEIX ## 9999: VOIPREUX 51130 MONSIEUR BRUNO BONNET ## 10000: MASSY 91300 MADAME TACKO THIAM ## adr_et_l2 adr_et_l3 ## 1: CYNOGUARD ## 2: ## 3: ## 4: ## 5: ## --- ## 9996: G.I.E. DES PHARMACIES DE CHAMPAGNE ## 9997: CHEMIN DE CHASSAGNE ## 9998: INTER POSE ## 9999: ## 10000: ## adr_et_l4 adr_et_l5 adr_et_l6 adr_et_l7 ## 1: LE HAUT CRUE 53160 JUBLAINS NA ## 2: LA GASSOTTE 86310 SAINT SAVIN NA ## 3: LIEU DIT CHANTUZIER 43300 VISSAC AUTEYRAC NA ## 4: 12 RUE DES FONTAINES 25680 TROUVANS NA ## 5: L&#39;EXTERNAT 07110 LAURAC EN VIVARAIS NA ## --- ## 9996: 24 B RUE PAUL GOERG 51130 VERTUS NA ## 9997: B CHE CHASSAGNE 24290 SAINT AMAND DE COLY NA ## 9998: 2 PL DE L ARSENAL 59800 LILLE NA ## 9999: 8 RUE DU PRESBYTERE 51130 VOIPREUX NA ## 10000: 6 SQ DE GRENOBLE 91300 MASSY NA ## nic_siege unite_type region adr_depcom region_impl region_mult ## 1: 15 1 52 53122 52 MULT ## 2: 14 1 75 86246 75 MONO ## 3: 18 1 84 43013 84 MONO ## 4: 20 1 27 25572 27 MONO ## 5: 27 1 84 07134 84 MULT ## --- ## 9996: 13 1 44 51612 44 MULT ## 9997: 16 1 75 24364 75 MONO ## 9998: 18 1 1 97132 1 MULT ## 9999: 11 1 44 51651 44 MULT ## 10000: 10 1 11 91377 11 MONO ## tr_eff_etp cj denom ## 1: 01 1200 ## 2: NN 1900 ## 3: NN 1500 ## 4: NN 1500 ## 5: NN 1600 ## --- ## 9996: NN 6220 GIE PHARMACIES DE CHAMPAGNE ## 9997: NN 1200 ## 9998: NN 1300 ## 9999: NN 1600 ## 10000: NN 1300 ## denom_condense sigle enseigne eff_3112_uniteLegale ## 1: 1 ## 2: NA ## 3: NA ## 4: NA ## 5: NA ## --- ## 9996: GIE PHARMACIES DE CHAMPAGNE NA ## 9997: NA ## 9998: NA ## 9999: NA ## 10000: NA ## eff_etp_uniteLegale eff_effet_daaaammjj_uniteLegale x ## 1: 1 20071231 443050.0 ## 2: NA NA 536650.0 ## 3: NA NA 752250.0 ## 4: NA NA 952412.5 ## 5: NA NA 801850.0 ## --- ## 9996: NA NA 773650.0 ## 9997: NA NA 564650.0 ## 9998: NA NA 704112.3 ## 9999: NA NA 776421.0 ## 10000: NA NA 648096.3 ## y SourceXYW qual x_cell y_cell cell_id w ## 1: 6801450 212.1320 8.0 443000 6801000 443000_6801000 0.7812752 ## 2: 6609750 380.7887 8.0 537000 6610000 537000_6610000 0.6613804 ## 3: 6446150 494.9747 8.5 752000 6446000 752000_6446000 0.0466952 ## 4: 6707939 NA 1.0 952000 6708000 952000_6708000 0.8298072 ## 5: 6379950 NA 9.0 802000 6380000 802000_6380000 0.4619149 ## --- ## 9996: 6867550 291.5476 8.5 774000 6868000 774000_6868000 0.3916708 ## 9997: 6441650 291.5476 8.5 565000 6442000 565000_6442000 0.8028523 ## 9998: 7060053 NA 8.0 704000 7060000 704000_7060000 0.7777200 ## 9999: 6868066 NA 1.0 776000 6868000 776000_6868000 0.6195679 ## 10000: 6848364 NA 1.0 648000 6848000 648000_6848000 0.3948781 ## w2 ## 1: 0.28129055 ## 2: 0.16139581 ## 3: -0.45328941 ## 4: 0.32982257 ## 5: -0.03806971 ## --- ## 9996: -0.10831383 ## 9997: 0.30286764 ## 9998: 0.27773541 ## 9999: 0.11958333 ## 10000: -0.10510655 .N qu’on a déjà rencontré, est une fonction construite pour retourner le nombre d’observations par groupe (les variables n’étant ici pas groupées, on renvoie le nombre de lignes). 4.4.1 Remplacement conditionnel Les requêtes sur la dimension i peuvent servir à créer de nouvelles colonnes. Dans ce cas, on peut penser la dimension i comme un update en SQL ou un case when profvis::profvis({ df[,&#39;iledefrance&#39; := FALSE] df[region %in% c(75, 77, 78, 91, 92, 93, 94, 95), iledefrance := TRUE] gc() df %&gt;% dplyr::mutate(region = dplyr::case_when( region %in% c(75, 77, 78, 91, 92, 93, 94, 95) ~ TRUE, TRUE ~ FALSE )) }) 4.4.2 Subsetting des colonnes 1 seule colonne: flights[, arr_delay] NSE ou flights[, get(‘arr_delay’)] SE flights[[“arr_delay”]] Select arr_delay column, but return as a data.table instead. ans &lt;- flights[, list(arr_delay)] We wrap the variables (column names) within list(), which ensures that a data.table is returned. In case of a single column name, not wrapping with list() returns a vector instead, data.table also allows wrapping columns with .() instead of list(). It is an alias to list(); they both mean the same. Select both arr_delay and dep_delay columns. ans &lt;- flights[, .(arr_delay, dep_delay)] head(ans) elect both arr_delay and dep_delay columns and rename them to delay_arr and delay_dep. Since .() is just an alias for list(), we can name columns as we would while creating a list. ans &lt;- flights[, .(delay_arr = arr_delay, delay_dep = dep_delay)] head(ans) Equivalent du select avec .SDcols select_cols = c(“arr_delay”, “dep_delay”) flights[ , ..select_cols] flights[ , select_cols, with = FALSE] For those familiar with the Unix terminal, the .. prefix should be reminiscent of the “up-one-level” command, which is analogous to what’s happening here – the .. signals to data.table to look for the select_cols variable “up-one-level”, i.e., in the global environment in this case. with = TRUE is the default in data.table because we can do much more by allowing j to handle expressions Update some rows of columns by reference - sub-assign by reference flights[hour == 24L, hour := 0L] Ce cours couvre de manière transversale les vignettes du package: “Introduction to data.table” ; “Keys and fast binary search based subset” ; “Reference semantics” : “Efficient reshaping using data.tables”. Les autres vignettes “Benchmarking data.table” et “Secondary indices and auto indexing” sont intéressantes également↩ On peut d’ailleurs y retrouver une réponse extensive d’Hadley Wickham (créateur dplyr) et des créateurs de data.table comparant et justifiant les deux approches.↩ A titre personnel, maintenant que j’y suis habitué, j’ai une préférence pour la syntaxe data.table après avoir été un grand utilisateur/amateur du tidyverse.↩ Si vous ne me croyez pas, essayer de comprendre du premier coup la vignette dplyr sur le sujet↩ On peut trouver ici une explication de la manière dont le terminal git peut se substituer, dans Rstudio au terminal cmd Windows. ↩ "],
["aggregations.html", "Chapter 5 Aggregations", " Chapter 5 Aggregations ans &lt;- flights[, sum( (arr_delay + dep_delay) &lt; 0 )] ans ans &lt;- flights[, .(.N), by = .(origin)] ans &lt;- flights[carrier == “AA”, .(mean(arr_delay), mean(dep_delay)), keyby = .(origin, dest, month)] ans All we did was to change by to keyby. This automatically orders the result by the grouping variables in increasing order. In fact, due to the internal implementation of by first requiring a sort before recovering the original table’s order, keyby is typically faster than by because it doesn’t require this second step. Keys: Actually keyby does a little more than just ordering. It also sets a key after ordering by setting an attribute called sorted. Can by accept expressions as well or does it just take columns? Yes it does. As an example, if we would like to find out how many flights started late but arrived early (or on time), started and arrived late etc… ans &lt;- flights[, .N, .(dep_delay&gt;0, arr_delay&gt;0)] ans Special symbol .SD: data.table provides a special symbol, called .SD. It stands for Subset of Data. It by itself is a data.table that holds the data for the current group defined using by. Recall that a data.table is internally a list as well with all its columns of equal length. Let’s use the data.table DT from before to get a glimpse of what .SD looks like. .SD contains all the columns except the grouping columns by default. To compute on (multiple) columns, we can then simply use the base R function lapply(). DT[, lapply(.SD, mean), by = ID] .SD holds the rows corresponding to columns a, b and c for that group. We compute the mean() on each of these columns using the already-familiar base function lapply(). Each group returns a list of three elements containing the mean value which will become the columns of the resulting data.table. Since lapply() returns a list, so there is no need to wrap it with an additional .() .SDcols Using the argument .SDcols. It accepts either column names or column indices. How can we return the first two rows for each month? ans &lt;- flights[, head(.SD, 2), by = month] head(ans) Parler de l’indexation "],
["utilisation-avancee.html", "Chapter 6 Utilisation avancée", " Chapter 6 Utilisation avancée "],
["fast-subsetting.html", "Chapter 7 Fast subsetting", " Chapter 7 Fast subsetting All data.frames have a row names attribute. We can subset a particular row using its row name. row names are more or less an index to rows of a data.frame. However, Each row is limited to exactly one row name. row names should be unique. data.tables never uses row names. Since data.tables inherit from data.frames, it still has the row names attribute. But it never uses them. Instead, in data.tables we set and use keys We can set keys on multiple columns and the column can be of different types Uniqueness is not enforced, i.e., duplicate key values are allowed. Since rows are sorted by key, any duplicates in the key columns will appear consecutively. Setting a key does two things: physically reorders the rows of the data.table by the column(s) provided by reference, always in increasing order. marks those columns as key columns by setting an attribute called sorted to the data.table. Note that we did not have to assign the result back to a variable. This is because like the := function we saw in the “Introduction to data.table” vignette, setkey() and setkeyv() modify the input data.table by reference. They return the result invisibly. set* and := Once you key a data.table by certain columns, you can subset by querying those key columns using the .() notation in i. flights[.(“JFK”)] The key column has already been set to origin. So it is sufficient to provide the value, here “JFK”, directly. The .() syntax helps identify that the task requires looking up the value “JFK” in the key column of data.table (here column origin of flights data.table). Subset all rows using key columns where first key column origin matches “JFK” and second key column dest matches “MIA” setkey(flights, origin, dest) flights[.(“JFK”, “MIA”)] flights[.(“LGA”, “TPA”), .(arr_delay)] We have seen so far how we can set and use keys to subset. But what’s the advantage? For example, instead of doing: "],
["key-by-origindest-columns.html", "Chapter 8 key by origin,dest columns", " Chapter 8 key by origin,dest columns flights[.(“JFK”, “MIA”)] we could have done: flights[origin == “JFK” &amp; dest == “MIA”] One advantage very likely is shorter syntax. But even more than that, binary search based subsets are incredibly fast. To use slow vector scan key needs to be removed. setkey(flights, NULL) flights[origin == “JFK” &amp; dest == “MIA”] The speed-up of indexes data.table is ~327x! Vector scan approach: The column x is searched for the value “g” row by row, on all 20 million of them. This results in a logical vector of size 20 million, with values TRUE, FALSE or NA corresponding to x’s value. Similarly, the column y is searched for 877 on all 20 million rows one by one, and stored in another logical vector. Element wise &amp; operations are performed on the intermediate logical vectors and all the rows where the expression evaluates to TRUE are returned. This is what we call a vector scan approach. And this is quite inefficient, especially on larger tables and when one needs repeated subsetting, because it has to scan through all the rows each time. Now let us look at binary search approach (method 2). Recall from Properties of key - setting keys reorders the data.table by key columns. Since the data is sorted, we don’t have to scan through the entire length of the column! We can instead use binary search to search a value in O(log n) as opposed to O(n) in case of vector scan approach, where n is the number of rows in the data.table. Since rows of each column of data.tables have contiguous locations in memory, the operations are performed in a very cache efficient manner (also contributes to speed). 8.0.1 Remplacement conditionnel très efficace A mettre dans data.table: beaucoup mieux d’updater les lignes par référence, de cette manière df1 &lt;- data.table::copy(df) df2 &lt;- data.table::copy(df) df3 &lt;- data.table::copy(df) ggplot2::autoplot(microbenchmark::microbenchmark( df1[!is.finite(get(&#39;wealth2009&#39;)), &#39;wealth2009&#39; := 0][], df2[,&#39;wealth2009&#39; := lest::if_else(!is.finite(get(&#39;wealth2009&#39;)),0,get(&#39;wealth2009&#39;))][], df3[,&#39;wealth2009&#39; := hutils::if_else(!is.finite(get(&#39;wealth2009&#39;)),0,get(&#39;wealth2009&#39;))][], times = 20 )) "],
["applications.html", "Chapter 9 Applications 9.1 Example one 9.2 Example two", " Chapter 9 Applications Some significant applications are demonstrated in this chapter. 9.1 Example one 9.2 Example two "],
["final-words.html", "Chapter 10 Final Words", " Chapter 10 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
