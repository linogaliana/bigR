# Fondamentaux de l'optimisation en R

<script src="./hideOutput.js"></script>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)
```


>  Premature optimization is the root of all evil (or at least most of it) in programming.
>
> Donald Knuth

### Introduction

L'objectif général du cours est d'offrir une introduction aux possibilités qu'offre `R` pour faire des calculs lourds et lire des bases de données volumineuses. Les deux principales ressources disponibles sur le sujet sont les ouvrages *Efficient R Programming* [@gillespie2016efficient] et *Advanced R* [@wickham2014advanced]^[Dans ce cours, on adoptera la vision de l'efficience au sens d'efficience algorithmique, i.e. la vitesse à laquelle un programme s'exécute et les ressources nécessaires pour l'accomplissement de celui-ci. Nous parlerons peu de la notion d'efficacité de programmation: quantité de travail utile qu'un programmeur peut réaliser par unité de temps. A cet égard, des astuces et règles utiles sont présentées dans la Formation *travail collaboratif sous R* et dans le livre de @gillespie2016efficient]. Cependant, ce cours approfondira des aspects superficiels de ces livres, notamment la puissance du package `data.table`.

Avant de chercher à optimiser, le premier réflexe à adopter est de bien structurer le problème que l'on vise à résoudre et sélectionner le format de données et les *packages* les plus adéquats. `R` étant un langage très malléable, il existe généralement plusieurs manières de résoudre un problème: avoir un code efficace nécessite en premier lieu de choisir le meilleur outil disponible en `R`. Par exemple, si on doit passer par une boucle, il vaut mieux choisir rapidement de faire du `Rcpp` plutôt qu'écrire une boucle `R` qu'on accélèrera par la suite. Un des objectifs de ce cours est de faire connaître les outils les plus adéquats pour des problèmes génériques. Plutôt qu'essayer de paralléliser sur 16 coeurs, il est parfois préférable de bien en utiliser un seul. Casser des cailloux à 16 est mieux que tout seul mais utiliser un outil adapté fait souvent gagner plus de temps.

Quand on se préoccupe d'efficacité ou de gestion de mémoire, le principal risque est la **dette technique**: un code pas très bon dont on reporte leà plus tard le moment de l'améliorer. Il s'agit, par exemple, d'un code qui utilise des boucles au lieu d'une opération vectorisée. Bien réfléchir à la structure du programme, à son découpage en parties homogènes, au format de données et aux packages mobilisables, est fondamental. C'est, probablement, le meilleur réflexe avant de chercher à optimiser.

Alors que le volume de données disponibles et pouvant être combinées est croissant, le ralentissement des progrès technologiques dans l'industrie des microprocesseurs (ralentissement de la <span style="color:royalblue">**loi de Moore</span>**^[Selon cette loi, le nombre de transistors composant un micro-processeur double tous les 18 mois à coût constant. Cela implique une capacité de calcul croissant à une grande vitesse. Cette observation empirique a été globalement respectée jusqu'en 2016, date depuis laquelle les progrès ont marqué le pas.]) justifie l'utilisation croissante de capacités de calculs parallèles. L'exploitation des systèmes multi-coeurs, qui sont maintenant communs, permet de s'abstraire, partiellement, de l'effet limitant du ralentissement de la loi de Moore sur les capacités de traitement. 


##### Plan du cours

La première étape nécessaire pour être en mesure d'être efficace en `R` et de comprendre le fonctionnement du langage. Comprendre la manière dont `R` fonctionne permet de comprendre l'intérêt de la vectorisation ainsi que la manière dont la mémoire fonctionne en `R`. Ce détour par le language de base permettra de mieux comprendre les raisons pour lesquelles `R` est un langage plus lent qu'un langage compilé et limité lorsqu'il est question de données volumineuses.

##### Packages nécessaires

Les packages que nous utiliserons principalement sont les suivants:

* `microbenchmark`
* `profvis`
* `foreach`
* `Rcpp`
* `data.table`

Pour pouvoir faire le comparatif `R`-`python` que vous pouvez retrouver dans cette leçon, il est nécessaire d'utiliser le package `reticulate` qui permet de faire interagir, dans une session `R` ou un `Rmarkdown`, les objets `R` et `python`. Si vous désirez exécuter les morceaux de code appopriés, qui sont là à des fins d'exemple, vous pouvez exécuter les commandes suivantes. Cela suppose néanmoins qu'une installation de `python` se trouve au chemin donné à la commande `Sys.setenv(RETICULATE_PYTHON = ..)`^[La manière la plus simple de disposer de `python` est d'utiliser [anaconda](https://www.anaconda.com/distribution/). Le container `pocker` que j'ai créé pour pouvoir utliser l'intégration continue avec `R` et `Python` pour compiler automatiquement ce `markdown` adopte cette approche]. 

<div class="fold s">
```{r check reticulate, echo=TRUE}
library(reticulate)

#if (Sys.getenv("LOCAL")==""){
  # CHANGE HERE TO YOUR PATH
#Sys.setenv(RETICULATE_PYTHON = "/opt/conda/envs/Reticulate/bin")

# CREATE A CONDA ENVIRONMENT JUST FOR RETICULATE
#reticulate::use_condaenv("Reticulate")

#reticulate::conda_install("Reticulate", "scipy")
#}
```
</div>


### Outils utiles pour mesurer l'exécution de fonctions

Nous allons utiliser deux types d'approches pour minuter la vitesse d'exécution d'un programme:

* **Benchmarking**: test de performance basé sur la répétition d'opérations
* **Profiling**: exécution d'une série de commande afin de déterminer les goulets d'étranglement

On utilise généralement la première approche lorsqu'on veut optimiser une opération pour laquelle plusieurs méthodes existent. La seconde approche permet de cibler les étapes d'un programme à cibler pour accélérer les calculs ou réduire l'utilisation de la mémoire.

L'utilisation de la CPU et de la RAM peut être contrôlée en temps réel avec le gestionnaire des tâches en Windows ou la commande `htop` sur `Linux`. Avec ces outils, on peut contrôler les processus en cours, l'utilisation des différents coeurs d'un ordinateur, la RAM utilisée et celle disponible. 


##### Microbenchmark

La fonction `system.time()` est la manière la plus simple de mesurer le temps d'exécution d'une ou plusieurs fonctions. Cependant, le temps d'exécution observé n'est qu'une réalisation d'une variable aléatoire et dépend de l'utilisation, à l'instant *t*, des ressources d'un ordinateur. Pour obtenir une image plus fidèle de la vitesse d'exécution d'une fonction, et être en mesure de comparer de manière plus adéquate des temps d'exécution, le package `microbenchmark` est très pratique. Comme les fonctions évaluées sont répétées de nombreuses fois (par défaut 100), on utilise `microbenchmark` pour des opérations rapides (quitte à utiliser les fonctions évaluées sur un échantillon plus petit de données). 

Par exemple, si on désire évaluer la performance des moyennes par groupe avec `dplyr`, `data.table` et en base `R`

<!-----
Exercice: Utiliser le package microbenchmark pour faire une moyenne par groupe du dataframe suivant

1. Avec une solution `R` base:  àggregate`
2. Avec une solution `dplyr`: `group_by` puis `summarise`
3. Avec une solution `data.table`: `dt[,mean(),by = ]`
----->

```{r first microbenchmark example, message = FALSE}
# On importe seulement le pipe, pas tout dplyr
import::from("magrittr","%>%")

# On crée un dataframe avec 5 catégories
df <- data.frame(x = rnorm(10e5),
                 y = sample(1:5,size = 10e5,
                            replace = TRUE))
# On crée son alter-ego data.table pour éviter de faire la conversion N fois
dt <- data.table::data.table(df)
# On indexe le data.table pour tirer profiter pleinement de la performance du package (cf. chapitre data.table)
data.table::setkeyv(dt,"y")



# Compare data.table, dplyr, base R
micro <- microbenchmark::microbenchmark(
  aggregate(df$x, by = list(df$y), FUN = mean),
  dt[,mean(x), by = y],
  df %>% dplyr::group_by(y) %>% dplyr::summarise(mean(x)),
  times = 20
)
print(micro)

ggplot2::autoplot(micro)
```

Nous reviendrons sur ce code dans le chapitre `data.table`. Ce benchmark nous indique déjà que l'approche `data.table` est plus rapide (l'ordre de grandeur est ici de l'ordre de 1 pour 4 par rapport à `dplyr` et 1 pour 40 par rapport à la solution R base.)

##### Profiling

Le *profiling* consiste à contrôler le temps d'exécution et l'usage mémoire d'une série de commandes. C'est une approche particulièrement bien adaptée à des codes modulaires (découpés sous formes de *chunks*) puisqu'on peut vraiment contrôler les parties de code les plus gourmandes en temps ou en mémoire. Nous aurons l'occasion, dans les exercices, d'appliquer plusieurs fois la fonction `profvis` du package du même nom 

```{r, eval = TRUE}
profvis::profvis(expr = {
  
  # Stage 1: load packages
  library("magrittr")
  
  # Stage 2: load and process data
  df <- data.frame(x = rnorm(10e5),
                    y = sample(1:5,size = 10e5, replace = TRUE))
  
  df2 <- df %>% dplyr::group_by(y) %>%
    dplyr::summarise(x2 = mean(x)) %>%
    dplyr::mutate(x2 = x2/mean(x2))
  
  df <- df %>% dplyr::left_join(df2)

  # Imaginons on fait une moyenne par groupe avec lapply
  # (nb: pas du tout la methode la plus efficace)
  # lapply(unique(df$y))

  # Stage 3: visualise output
  p <- ggplot2::ggplot(df,
                  ggplot2::aes(x = factor(y), y = x)
                  ) +
    ggplot2::geom_boxplot(ggplot2::aes(colour = factor(y))) +
    ggplot2::theme_bw()
  
  print(p)
  
})


```

Ce package offre une belle représentation du temps passé et des usages mémoires pour chaque bout de programme. C'est particulièrement utile pour déterminer les goulets d'étranglement, et donc les morceaux à optimiser, d'un programme. Avec cet exemple, on voit que la majorité du temps est passée dans la représentation graphique, étape la plus longue et la plus exigente en mémoire. 


**Exercice:**

Faire un `microbenchmark` d'une somme cumulative avec:

1. Une boucle for
2. Une solution utilisant `sapply`
3. La fonction `cumsum`

Vous pouvez créer un vecteur `x <- seq_len(100)` pour faire ce test

<div class="fold s">
```{r}
x <- seq_len(100) # initiate vector to cumulatively sum

# Method 1: with a for loop (10 lines)
cs_for = function(x){
  for(i in x){
    if(i == 1){
      xc = x[i]
    } else {
      xc = c(xc, sum(x[1:i]))
    }
  }
  xc
}

# Method 2: with apply (3 lines)
cs_apply = function(x){
  sapply(x, function(x) sum(1:x))
}

# Method 3: cumsum (1 line, not shown)
print(
  microbenchmark::microbenchmark(cs_for(x), cs_apply(x), cumsum(x))
)

```
</div>

Vous pouvez apprécier la différence de vitesse entre un code vectorisé (`cumsum`) et des approches plus artisanales, qui sont également moins lisibles. 

### Comprendre les ordinateurs pour comprendre `R`

Un ordinateur est une réalisation concrête d’une machine de Turing, c'est-à-dire une
machine traitant des informations et capable en principe de prendre comme donnée n’importe quel algorithme et de l’exécuter.

##### Principes généraux

Les deux principaux constituants d’un ordinateur sont la **mémoire principale** et le **processeur** ou **CPU** (Central Processing Unit). La mémoire principale permet de stocker de l’information (programmes et données), tandis que le processeur exécute pas à pas les instructions composant les programmes.


L'interaction entre les différentes composantes d'un ordinateur suit l’architecture de Von-Neumann:

1. Le <span style="color:royalblue">**processeur (CPU pour Central Processing Unit)**</span> est conceptuellement décomposé en:
    + une *unitée de contrôle (UC)*: séquence les opérations
    + une *unitée de calcul arithmétique et logique (UAL)*: exécution des opérations de base
    + un *registre*
2. La <span style="color:royalblue">**mémoire**</span>: contient à la fois les données et le programme
exécuté par l’unité de contrôle
    + *Mémoire vive ou volatile (RAM pour random access memory)*: contient programmes et données en cours de traitement
    + *Mémoire permanente (ROM pour read only memory)*: stocke programmes et données de base de la machine
3. <span style="color:royalblue">**Dispositifs d'entrée-sortie (input/output)**</span>: périphériques qui permettent de communiquer avec le monde extérieur
4. <span style="color:royalblue">**Canaux de communication (les bus)**</span> entre la mémoire, le processeur et les périphériques


![](./pics/architecture_von_neumann.png)

La **carte mère** est le support physique permettant la coordination de l’ensemble des éléments intervenant dans le fonctionnement d’un ordinateur.

Si l’architecture de Von-Neumann se caractérise par sa grande souplesse, elle présente en revanche trois gros inconvénients :

1. Une *exécution séquentielle*. L’architecture de Von Neumann impose une exécution séquentielle des instructions. Avec ce modèle, il n’est donc pas possible d’effectuer différents travaux en parallèle sur un même processus.
2. Un *goulet d’étranglement*. Alors que le microprocesseur peut exécuter des instructions très rapidement, celles-ci sont de toute façon ralenties par la vitesse de transfert des informations dans les bus de communication.
3. *Faible robustesse*. Le fait que les données et les programmes soient mélangés engendre une fragilité du modéle de Von-Neumann : si une donnée est enregistrée à l’emplacement d’un programme, le comportement de l’ordinateur peut devenir complètement incohérent.

Le **système d'exploitation** dirige l'utilisation des ressources d'un ordinateur. Cela donne l'illusion que de nombreux processus peuvent être exécutés en même temps. Cela provient de la conjonction de deux effets : d’une part, les instructions exécutées par l’ordinateur sont extrêmement rapides (de l’ordre de quelques dizaines de millisecondes) et d’autre part, le système d’exploitation utilise les temps d’attente dans l’exécution d’une tâche pour orienter les ressources de l’ordinateur (micro-processeur + RAM...) vers l’exécution d’une autre tâche. Le fait que la plupart des ordinateurs soient maintenant dotés de plusieurs processeurs permet, si les applications sont adaptées, d’exécuter réellement plusieurs tâches simultanément. Malgré tout, le nombre d’instructions effectuées simultanément reste habituellement très inférieur au nombre d’applications en fonctionnement simultané.

##### Langage interprété vs compilé

`R` est un langage interprété (comme `python`), par opposition aux langages compilés (`C++` par exemple). Par rapport à l’interpréteur, le compilateur présente l’inconvénient de retarder l’exécution d’un programme mais une fois compilé, le fichier exécutable obtenu est particulièrement rapide (moins de surcouche vers le langage machine).

Certains packages (par exemple `compiler`) proposent d'accélérer les traitements en transformant des exécutions interprétées (du code `R` standard) en instructions compilées (`bytecode`). C'est une méthode, assez simple puisque c'est le package qui gère cette traduction, pour accélérer certains programmes. Depuis `R 3.5.0` (avril 2018), les fonctions intégrées à des packages sont automatiquement compilées, ce qui accélère les temps d'exécution sans effort de programmation. 


#### Le processeur (CPU)


Le processeur (ou micro-processeur) est considéré comme le coeur de l’ordinateur. C’est lui qui attribut des tâches simples aux périphériques et à la RAM tandis qu’il exécute les tâches les plus compliquées. Il est responsable de l’exécution d’un programme. Le processeur est un circuit éléctronique complexe (circuit intégré) qui exécute chaque instruction très rapidement, en quelques **cycles d’horloges**. Toute l’activité de l’ordinateur est cadencée par une horloge unique, de façon à ce que tous les circuits électroniques travaillent tous ensemble de façon synchronisée. La fréquence de cette horloge s’exprime en `MHz` (millions de cyles par seconde) ou `GHz` (milliards de cycles par secondes). Par exemple, le processeur *Intel Core i5-6300U* qui équipe les postes nomades INSEE possède une horloge cadencée à 2,40 GHz.

Un processeur est défini, entre autres, par :

1. La largeur de ses registres internes de manipulation de données. Les registres sont des mémoires de petite taille (quelques octets), suffisamment rapides pour que l’UAL puisse manipuler leur contenu à chaque cycle de l’horloge. Les deux largeurs les plus communes sont 32 et 64 bits.
2. La cadence de son horloge exprimée en MHz ou GHz ;
3. Le nombre de noyaux de calcul (core). Le chapitre sur la parallélisation reviendra sur ce point.

Le processeur est principalement divisé en deux parties

1. L’unité de contrôle: responsable de la lecture en mémoire principale et du décodage des instructions ;
2. L’unité de calcul arithmétique et logique (UAL): exécute les instructions qui manipulent les données. C’est elle qui effectue les opérations algébriques (somme, différence, produit, rapport) et logiques (disjonction, conjonction, négation) usuelles

Ces deux unités communiquent avec la mémoire principale, la première pour lire les instructions, la seconde pour recevoir/transmettre des données binaires, mais ils communiquent également avec les différents périphériques (clavier, souris, écran, etc.).


<!--------
Les instructions que doit suivre l’unité de contrôle sont elles aussi inscrites dans la RAM. Pour les exécuter, l’unité de contrôle utilise un registre particulier appelé PC (Program Counter) et exécute en boucle la séquence d’actions suivantes :

  * Lire instruction : Aller lire le mot stocké à l’adresse mémoire inscrite dans le registre PC et le stocker dans un registre spécial appelé IR (Registre d’Instruction). Les instructions sont des mots mémoires codés en
binaire que l’on appelle le langage machine.
  * Incrémenter PC : Ajouter 1 au mot stocké dans le registre PC et enregistrer le résultat dans ce même registre.
  * Décoder instruction : Décoder l’instruction contenue dans IR, c’est à dire, reconnaître s’il s’agit d’une
opération arithmétique et logique, d’un accès à la mémoire vive ou d’un branchement (voir plus loin).
  * Exécuter instruction : Exécuter l’instuction décodée.
  
Le programme est représenté par une série d’instructions qui réalisent
des opérations en liaison avec la mémoire vive de l’ordinateur. Il y a
quatre étapes lors du traitement des instructions (architecture de Von Neumann):
1 FETCH : Recherche de l’instruction ;
2 DECODE : Décodage de l’instruction ;
3 EXECUTE : Exécution des opérations ;
4 WRITEBACK : Écriture du résultats.
  
------>

#### La mémoire vive ou RAM (random access memory)

La mémoire est divisée en bytes qui sont des emplacements (des cases mémoires contiguës) de taille fixe utilisés pour stocker instructions et données. Il est facile de confondre les termes `bit` et `byte`: un `bit` (contraction de *binary digit*) est l'élement de base de l'ordinateur, le `byte` stocke plusieurs éléments.
En principe, la taille d’un emplacement mémoire pourrait être quelconque ; en fait, la plupart des ordinateurs en service aujourd’hui utilisent des emplacements mémoire d’un octet (soit 8 bits)^[Aujourd'hui, les termes bytes et octets sont utilisés de manière relativement indistincte même si, conceptuellement, il s'agit de notions différentes. Le byte est le nombre de bits nécessaire pour coder un caractère. Il s'agit ainsi de la plus petite unité logique adressable par un programme sur un ordinateur. Les bytes de 8 bits se sont généralisés dans les ordinateurs modernes même si, conceptuellement, les bytes pouvaient être de longueur différente. Par le passé, les caractères ASCII dominaient l'informatique. Il s'agissait d'un ensemble de 128 caractères qui ne nécessitaient que 7 bits ($2^7=128$) mais en utilisaient 8 pour optimiser la performance. La définition d'un octet est d'être une unité composée de 8 bits. C'est la généralisation des bytes à 8 bits qui explique la correspondance entre ces deux termes. En informatique, si l'on veut explicitement désigner une quantité de huit bits, on utilise le mot octet ; tandis que si l'on veut exprimer l'unité d'adressage indépendamment du nombre de bits, on utilise le mot byte. ]. Les séquences de bytes sont elles-mêmes regroupées en mots mémoire dont la taille est déterminée par celle des registres de l'ordinateur (32 ou 64 bits)^[Les processeurs 32 bits ne peuvent normalement pas adresser plus de 4 Gio ($2^{32}$ octets) de mémoire centrale, tandis que les processeurs 64 bits peuvent en adresser 16 Eio ($2^{64}$ octets). C'est pourquoi dès qu'il y a plus de 4 Gio de RAM sur une machine, la mémoire au-delà de ce seuil ne sera directement adressable qu'en mode 64 bits].

<!---Il existe au moins 3 types de mémoire vive : SD, SSD et DDR-SDRAM (d’accès plus rapide).---->


Bit representation | Caractère
-------------------|-----------
01000001           | A
01000010           | B
01000011           | C



La RAM possède les caractéristiques suivantes :

1. Un mot dans la mémoire peut aussi bien représenter une instruction dans un programme, qu’un entier ou une couleur selon l’interprétation que lui donne l’utilisateur.
2. Elle est inerte dans le sens où elle sert juste de support de stockage de l’information.
3. Tout mot mémoire possède une adresse (codé sous la forme d’un entier) lui permettant d’être lu rapidement par le microprocesseur.

Lorsque la mémoire vive arrive à saturation en raison d’une utilisation instantanée intensive de l’ordinateur, une partie du disque dur peut-être utilisée par le microprocesseur : on appelle cela le *swap*. Dans ce cas, le temps d’exécution des instructions est beaucoup plus lent (de l’ordre de 1000 fois).

<!----------
Seul le processeur peut modifier l’état de la mémoire. Chaque emplacement mémoire conserve les informations que le processeur y écrit jusqu’à coupure de l’alimentation électrique, où tout le contenu est perdu (contrairement au contenu des mémoires externes comme
les disquettes et disques durs). On parle de mémoire vive. Les seules
opérations possibles sur la mémoire sont :
1 écriture d’un emplacement : le processeur donne une valeur et
une adresse, et la mémoire range la valeur à l’emplacement
indiqué par l’adresse ;
2 lecture d’un emplacement : le processeur demande à la mémoire
la valeur contenue à l’emplacement dont il indique l’adresse. Le
contenu de l’emplacement auquel le processeur accède en lecture
demeure inchangé.

Caractéristiques d’une mémoire
1 La capacité : nombre total de bits que contient la mémoire. Elle
s’exprime aussi souvent en octet ;
2 Le format des données : nombre de bits que l’on peut mémoriser
par case mémoire. On parle de la largeur du mot mémorisable ;
3 Le temps d’accès : temps qui s’écoule entre l’instant où a été
lancée une opération de lecture/écriture en mémoire et l’instant
où la première information est disponible sur le bus de données ;
4 Le temps de cycle : il représente l’intervalle minimum qui doit
séparer deux demandes successives de lecture ou d’écriture ;
5 Le débit : nombre maximum d’informations lues ou écrites par
seconde ;
6 La volatilité : elle caractérise la permanence des informations
dans la mémoire. L’information stockée est volatile si elle risque
d’être altérée par un défaut d’alimentation électrique et non
volatile dans le cas contraire.
------------>


<!-------------
Les instructions et les données transmises au processeur sont
exprimées en mots binaires (code machine). Elles sont stockées dans
la mémoire. Le séquenceur ordonne la lecture du contenu de la
mémoire et la constitution des mots présentées à l’UAL qui les
interprète. L’ensemble des instructions et des données constitue un
programme. Le langage le plus proche du code machine tout en
restant lisible par des humains est le langage d’assemblage, aussi
appelé langage assembleur
------------------>


#### Comparer la performance de R par rapport à d'autres utilisateurs

Le package `benchmarkme` est conçu pour faire tourner quelques calculs standardisés afin de comparer la performance de `R` à d'autres utilisateurs qui ont accepté de partager la performance de leur ordinateur. 

```{r, eval = FALSE}
res = benchmarkme::benchmark_std() 
```

Pour se comparer aux autres utilisateurs: 

```{r, eval = FALSE}
plot(res)
```

![](./pics/benchmarkme1.png)
![](./pics/benchmarkme2.png)
![](./pics/benchmarkme3.png)

Le poste utilisé pour les tests a des performances médianes: sans être catastrophique, il n'est pas non plus exceptionnel.

### Retour à `R`: appels de fonctions

> To understand computations in R, two slogans are helpful:
>
>  * Everything that exists is an object.
>  * Everything that happens is a function call.
>
> *John Chambers*

Appeler une function `R` revient en fait à appeler, de manière plus ou moins directe, un code `C` ou `Fortran` Les fonctions de base sont généralement des appels implicites à une fonction `C` sous-jacente. Par exemple, la fonction `runif()` contient une unique ligne qui est un appel à la fonction  `C_runif()`

```{r runif C call}
body(runif)
```

Les fonctions développées en `C++` grâce au package `Rcpp` fonctionnent de la même manière. Par exemple, la fonction que j'ai développée 

```{r}
dplyr::between
```
fonctionne comme un *wrapper* d'une fonction sous-jacente en `C++` qui est, elle-même, stockée dans un fichier compilé `.dll`

```{r}
dplyr:::`_dplyr_between`
```

Pour obtenir de bonnes performances en `R`, il est important d'accéder aux routines sous-jacentes `C` ou `Fortran` le plus rapidement possible. Appeler des fonctions ayant un coût en termes de temps, il est important de minimiser, autant que possible, le nombre d'appels pour accéder aux fonctions `C` ou `Fortran`.

Ce principe explique la lenteur des boucles en `R`. Par exemple, en supposant que `x` soit un vecteur standard, 

```{r, eval = FALSE}
# ex: x <- runif(10)
x = x + 1
```

ne nécessite qu'un appel à la fonction `+`. 

La boucle `for` équivalente

```{r for loop, eval = FALSE}
for(i in seq_len(n)) 
  x[i] <- x[i] + 1 
```

nécessite

* *n* appels à la fonction `+`
* *n* appels à la fonction `[` ;
* *n* appels à la fonction `[<-` (assignation);
* 2 appels supplémentaires: un à la fonction `for` et l'autre `seq_len()`.

En soi, la boucle *for* n'est pas longue mais les appels de fonctions sont bien trop nombreux pour la rendre compétitive. C'est parce que les appels de fonctions sous `R` impliquent des sur-couches par rapport à un langage compilé comme `C` ou `C++` qui accède plus directement au langage machine. Les méthodes de compilation à la volée (*just-in-time compilation*) telles qu'implémentées par le package `compiler` permettent d'accélérer les boucles mais il reste toujours préférable d'adopter une approche plus adaptée à R ou, si c'est impossible, de privilégier une boucle `C++` [@eddelbuettel2011rcpp].



<!---- Comme nous le montrerons par la suite, si la vectorisation n'est pas possible, il est préférable d'utiliser la fonction `lapply` que la fonction `for`.---->


**Exercice**

Utilisez le package `microbenchmark` pour comparer la solution de l'addition vectorielle à la version boucle. Faites la comparaison en variant les tailles de vecteur (conseil: créez deux fonction `plus_one` et `plus_one_for` prenant comme paramètre `n` qui est utilisé pour générer une variable aléatoire de taille `n`)

<div class="fold s">
```{r microbenchmark vectorisation}
plus_one <- function(n){
  x <- runif(n)
  x <- x+1
  return("OK")
}

plus_one_for <- function(n){
  
  x <- runif(n)
  for(i in seq_len(n)) x[i] = x[i] + 1 
  return("OK")
  
  return(x)
}

results_plus_one <- microbenchmark::microbenchmark(
  plus_one(10^2),
  plus_one(10^3),
  plus_one(10^4),
  plus_one(10^5),
  plus_one_for(10^2),
  plus_one_for(10^3),
  plus_one_for(10^4),
  plus_one_for(10^5),
  times = 50
  )

ggplot2::autoplot(results_plus_one)
```
</div>

##### Aparté: comparaison des boucles avec Python et C++

De nombreux tests de performance sont disponibles en ligne, notamment sur cette [page](https://h2oai.github.io/db-benchmark/). On propose un test plus modeste de comparaison des performances de `Python`, `R` et `C++`. Pour exécuter du code `Python` depuis `R`, on utilise le package `Reticulate`^[Pour les personnes utilisant l'intégration continue, cela nécessite d'adapter le dépôt `rocker` généralement utilisé. Un exemple que je propose est le dépôt `pocker`, disponible sur [dockerhub](https://cloud.docker.com/u/linogaliana/repository/docker/linogaliana/pocker) [github](https://github.com/linogaliana/pocker), [gitlab](https://gitlab.com/linogaliana/pocker). Pour une explication de la raison d'être d'une image `docker` de ce type, ce [post](https://linogaliana.netlify.com/post/pocker/pocker-a-docker-container-to-use-r-and-python-together/)].


```{r}
library(reticulate)
```

On se propose d'étudier la vitesse de ces langages sur l'opération très simple déjà évoquée `x <- x+1`. En `Python`, cela peut s'écrire:


```{python, eval = TRUE}

#conda_create("r-reticulate")
#reticulate::conda_install("r-reticulate", "scipy")

import numpy as np
import time
import statistics

# VERSION BOUCLE
def test(n = 1000): 
  start = time.time()
  x = np.random.uniform(0,1,n)
  for i in range(n):
    x[i] = x[i] + 1
  end = time.time()
  return((end - start)*100000000)  # Nanoseconds

exec_time_python = [test(100000) for k in range(100)]

# VERSION VECTORISEE
def test2(n = 1000):
  start = time.time()
  x = np.random.uniform(0,1,n) + 1
  end = time.time()
  return((end - start)*100000000)  # Nanoseconds

exec_time_python2 = [test2(100000) for k in range(100)]

```


A noter que l'utilisation de `reticulate` peut induire un surcoût par rapport à une utilisation de `Python` dans un environnement dédié (`Pycharm`, `jupyter`...). La solution équivalente en `R ` est


```{r}
# VERSION BOUCLE
test <- function(n = 1000){
  start <- Sys.time()
  x <- runif(n)
  for (i in seq_len(n)){x[i] <- x[i] + 1}
  end = Sys.time()
  return((end - start)*100000000) # Nanoseconds
}

# VERSION VECTORISEE
test2 <- function(n = 1000){
  start <- Sys.time()
  x <- runif(n) + 1
  end = Sys.time()
  return((end - start)*100000000)  # Nanoseconds
}

exec_time_R <- replicate(100, test(100000))
# median(exec_time_R*1000)
exec_time_R2 <- replicate(100, test2(100000))
```


La traduction `C++` est presque immédiate grâce à `Rcpp` (cf. chapitre dédié):


```{Rcpp, eval = TRUE}
#include <Rcpp.h>
using namespace Rcpp;
#include <Rcpp/Benchmark/Timer.h>

// [[Rcpp::export]]
int test_cpp(int n) {
  
    // start the timer
  Timer timer;
  timer.step("start");        // record the starting point
  
  NumericVector x = runif(n);
  
  for (int i=0; i < n; ++i) {
    x[i] += 1;
  }

  timer.step("end");   // record the final step    
  
  
  NumericVector res(timer);   // 
  for (int i=0; i<res.size(); i++) {
    res[i] = res[i] / n;
  }
  return (res[1]-res[0]); // Nanoseconds    
  
  //return x;
}
```


```{r, eval = TRUE}
exec_time_cpp <- replicate(100, test_cpp(100000))
```

On peut reproduire un graphique proche de celui produit par `microbenchmark` pour déterminer la vitesse d'exécution des différentes approches:

```{r}

exec_times <- do.call(rbind,list(
  data.frame(t = py$exec_time_python, langage = "python [loop]"),
  data.frame(t = exec_time_R, langage = "R [loop]"),
  data.frame(t = py$exec_time_python2, langage = "python"),
  data.frame(t = exec_time_R2, langage = "R [base]"),
  data.frame(t = exec_time_cpp, langage = "Rcpp")
))
ggplot2::ggplot(data = exec_times) + ggplot2::geom_violin(ggplot2::aes(x = langage, y = log(t))) +
  ggplot2::labs(y = "Iteration time in (log) nanoseconds")
```



### Mémoire

`R` est un langage qui a une utilisation intensive de la RAM. C'est ce qui permet au langage d'être assez rapide mais créé des difficultés lorsque les données sont volumineuses dans un environnement où la RAM est contrainte (les postes nomades de l'INSEE ont 8Go de RAM ; les ressources d'un serveur sont partagées entre tous les utilisateurs). Dans cette partie, nous verrons qu'outre l'import de données, la gourmandise de `R` provient de la manière dont les objets sont copiés en mémoire lorsqu'ils sont modifiés.

Le livre de @wickham2014advanced est une ressource précieuse sur le sujet. De nombreux exemples de cette section sont tirés de cet ouvrage. On utilisera le package `pryr` en supplément de `profvis` pour contrôler l'usage mémoire

#### Bases de la mémoire en `R`

Pour comprendre les besoins mémoire de `R`, on peut exécuter le code suivant qui inspecte la taille mémoire d'un vecteur en fonction de sa longueur:

```{r}
sizes <- sapply(0:50, function(n) pryr::object_size(seq_len(n)))
plot(0:50, sizes, xlab = "Length", ylab = "Size (bytes)", 
  type = "s")
```

La première conclusion est que même un objet vide occupe 40 bytes de mémoire (soit $40 \times 8 = 320$ bits). La seconde est que le besoin mémoire de `R` n'est pas tout à fait linéaire: la progression est en escalier. C'est parce que demander au système de la mémoire (appel caché à la fonction `malloc()`) est une opération coûteuse. Demander la mémoire à la volée ralentirait profondément `R`. A la place, `R` demande de la mémoire par bloc et organise ceux-ci tant qu'ils ne sont pas pleins. Passé une taille critique (128 bytes), `R` arrêt de demander des emplacements en puissance de 2 (8, 16, 32, 48, 64, 128) pour être plus flexible. 

`R` gère lui-même la mémoire que le système lui alloue. Il le fait de manière à être le plus parcimonieux possible. Cela se retrouve dans le comportement de copie à l'identique. Un même emplacement peut être partagé par plusieurs objets, ce qui limite l'inflation en mémoire:

```{r}
x <- 1:1e6
pryr::object_size(x)

y <- list(x, x, x)
pryr::object_size(y)
```

`y` ne pèse pas trois fois plus que `x`. `R` fait pointer `y` vers `x` plutôt que le copier. Cet usage particulier de la mémoire est, comme nous le verrons, une des raisons de l'efficience des assignations par référence et donc du package `data.table`. 

Le même comportement s'observe avec les variables de type `character` car `R` a une gestion globale de celles-ci. Cela signifie que chaque `string` unique est stockée uniquement a un endroit et permet ainsi aux vecteurs de texte, qui ont potentiellement des valeurs redondantes, de consommer moins de mémoire:

```{r}
pryr::object_size("banana")

pryr::object_size(rep("banana", 10))
```


#### Utilisation de la mémoire

`pryr::mem_used()` permet de connaître la mémoire consommée par l'ensemble des objets `R`. La consommation de mémoire totale est néanmoins différente car cela n'inclut que les objets stockés actuellement en mémoire et pas l'ensemble de la mémoire qui a été allouée par `R`. Par exemple,  si un objet volumineux a été supprimé, `R` ne rend pas nécessairement la mémoire libérée à l'OS. De même, ce chiffre n'incorpore pas le poids mémoire de l'interpréteur `R` (ni de l'IDE utilisé, par exemple `Rstudio`). Pour connaître le poids mémoire effectif, il faut utiliser `htop` (Linux) ou le gestionnaire de tâches de Windows.   

```{r}
pryr::mem_used()
```


Dans certains langages, il est nécessaire d'expliciter supprimer des objets pour que leur espace mémoire soit libéré. `R` adopte une approche différence, celle du **garbage collection** (GC). GC libère automatiquement la mémoire occupée par un objet qui n'existe plus (en inspectant les espaces qui pointent vers des noms d'objets n'existant pas). 


```{r}
# Create a big object
pryr::mem_change(x <- 1:1e6)

# Also point to 1:1e6 from y
pryr::mem_change(y <- x)

# Remove x, no memory freed because y is still pointing to it
pryr::mem_change(rm(x))

# Now nothing points to it and the memory can be freed
pryr::mem_change(rm(y))

```

Les problèmes de saturation de la RAM se traduisent généralement par un message du type `cannot allocate a vector of size ** Mb` (quand la session ne *crash* pas...). Même si `R` effectue des GC fréquemment, il me semble nécessaire d'appeler la commande `gc()` de temps en temps (en particulier après l'appel d'une fonction ayant effectué un gros traitement) pour rendre de la RAM à l'OS. Sur un serveur partagé, cela permet de réduire les risques de saturation à cause de sessions inactives qui n'ont plus besoin de la mémoire qu'elles mobilisent^[@wickham2014advanced dans son ouvrage développe l'idée que la commande `gc()` est inutile car la GC étant automatique, évoquer `gc()` ne libère aucun espace mémoire mais prend du temps. En pratique, je trouve que c'est faux (`gc()` permet souvent de libérer plusieurs `gigas`) et que sur un serveur partagé, c'est une règle de bonne conduite pour permettre à chacun d'avoir des chances de disposer de mémoire quand il en a besoin.]. 


#### Modification d'objets

Deux manières de modifier un objet sont possibles en `R`:

* **In-place modification**: `R` modifie `x` en place. Cela signifie que le nouvel objet prend la place mémoire de l'ancien. C'est une approche économe en mémoire. Les modifications de `data.table` sur lesquels nous reviendrons sont beaucoup plus efficaces et économes en mémoire que celles sur un `tibble` ou un `data.frame` parce que ce package est construit sur la modification par référence qui étend la modification en place.
* **Copy-on-modify**: `R` effectue une copie temporaire de `x` vers une nouvelle adresse, modifie la copie et ensuite pointe le nom de `x` vers le nouvel emplacement.

Selon les circonstances, R va faire de la modification en place ou en copie. Les fonctions de remplacement primitives (`[[<-, [<-, @<-, $<-, attr<-, attributes<-, class<-, dim<-, dimnames<-, names<-, and levels<-`) modifie en place. Ce n'est pas le cas des autres fonctions d'assignation: `[<-.data.frame` n'étant pas une fonction primitive, elle effectuera de la modification en copie. 

**Exercice**

Pour comprendre la différence entre ces deux approches, on va utiliser les fonctions `pryr::address` (qui donne le pointeur vers l'emplacement mémoire) et `tracemem` qui suit les changements d'adresse d'un objet.

1. Créez un vecteur `x <- 1:10` et une copie `y` de celui-ci. 
2. Utilisez `pryr::address` et `pryr::object_size`: quel enseignement en tire-t-on ?
3. Modifiez un élément du vecteur tout en utilisant `pryr::mem_used`. La modification sera `x[5] <- 6L`. Refaites appel à la fonction `pryr::address` et à la fonction `pryr::object_size`: quelle conclusion en tirez-vous sur la place de l'objet `y` en mémoire ? A-t-on fait de la modification en place ou en copie ?
4. Faites un appel `tracemem(x)` et modifiez deux fois d'affilée une valeur du vecteur `x`: qu'observez-vous ? 

<div class="fold s">
```{r, results = "hide"}
# Question 1
x <- 1:10
y <- x

# Question 2
c(pryr::address(x), pryr::address(y))
# [1] "0xfa05510" "0xfa05510"
pryr::object_size(x,y)
# 96 B

# Question 3
pryr::mem_change(x[5] <- 6L)
# -6.52 kB
c(pryr::address(x), pryr::address(y))
# "0x1d8380f0" "0xfa05510" 
pryr::object_size(x,y)
# 192 B

# Question 4
tracemem(x)
x[5] <- 6L
# tracemem[0x000000001d8380f0 -> 0x000000001d721648]:
x[5] <- 6L
# tracemem[0x000000001d721648 -> 0x000000001d6ef600]:
```
</div>

Comme @wickham2014advanced l'évoque, il est compliqué d'éviter les modifications par copie. S'il est nécessaire de les éviter pour des raisons de vitesse ou de mémoire, utiliser `Rcpp` ou `data.table` peut s'imposer.

Nous avons déjà évoqué un facteur à l'origine des boucles `for`: les appels répétés aux fonctions. Néanmoins, la raison principale de la lenteur des boucles provient généralement du comportement de copie de `R`. Lorsqu'on effectue une boucle sur un `data.frame`, chaque itération de la boucle copie cet objet. 


**Exercice**

1. Créer un dataframe `x <- data.frame(matrix(runif(100 * 1e4), ncol = 5))` et un vecteur stockant la médiane par colonne `medians <- vapply(x, median, numeric(1))`
2. Utiliser la fonction 
3. Faire une boucle for de 1 à 5:
    + Prend la colonne `i` et retranche à chaque élément du vecteur la médiane.
    + Réassigner le résultat à la colonne
Observez l'adresse de `x`

<div class="fold s">
```{r, results="hide"}
x <- data.frame(matrix(runif(100 * 1e4), ncol = 5))
medians <- vapply(x, median, numeric(1))

tracemem(x)
# "<00000000090AA718>"
for(i in 1:5) {
  x[, i] <- x[, i] - medians[i]
}
# tracemem[0x00000000090aa718 -> 0x0000000014f73ae0]: 
# tracemem[0x0000000014f73ae0 -> 0x0000000014f73990]: [<-.data.frame [<- 
# tracemem[0x0000000014f73990 -> 0x0000000014f73920]: [<-.data.frame [<- 
# tracemem[0x0000000014f73920 -> 0x0000000014f738b0]: 
# tracemem[0x0000000014f738b0 -> 0x0000000014f73760]: [<-.data.frame [<- 
# tracemem[0x0000000014f73760 -> 0x0000000014f736f0]: [<-.data.frame [<- 
# tracemem[0x0000000014f736f0 -> 0x0000000014f73680]: 
# tracemem[0x0000000014f73680 -> 0x0000000014f73530]: [<-.data.frame [<- 
# tracemem[0x0000000014f73530 -> 0x0000000014f734c0]: [<-.data.frame [<- 
# tracemem[0x0000000014f734c0 -> 0x0000000014f73450]: 
# tracemem[0x0000000014f73450 -> 0x0000000014f73300]: [<-.data.frame [<- 
# tracemem[0x0000000014f73300 -> 0x0000000014f73290]: [<-.data.frame [<- 
# tracemem[0x0000000014f73290 -> 0x0000000014f73220]: 
# tracemem[0x0000000014f73220 -> 0x0000000014f730d0]: [<-.data.frame [<- 
# tracemem[0x0000000014f730d0 -> 0x0000000014f73060]: [<-.data.frame [<-
```
</div>

Comme vous pouvez le voir, le `data.frame` est copié à chaque itération, et cela plusieurs fois. C'est parce que la fonction `[<-.data.frame` n'est pas une fonction primitive. En utilisant des listes, on peut être beaucoup plus efficace

**Exercice**

1. Créer `y` une copie de `x` sous forme de liste et faire `tracemem(y)`. Vous pouvez garder le vecteur précédent `medians`
2. Refaire la même chose que précédemment avec `y`
3. Observer et conclure

<div class="fold s">
```{r, results="hide"}
y <- as.list(x)

tracemem(y)
# "<0000000014FEAB30>"
for(i in 1:5) {
  y[[i]] <- y[[i]] - medians[i]
}
# tracemem[0x0000000014feab30 -> 0x00000000122a3600]:
```
</div>

#### Pré-allocation de la mémoire

A cause du comportement de copie, il faut faire attention à l'allocation de mémoire lorsqu'on désire ajouter des éléments à un objet. Comme nous allons le voir, il est préférable de pré-allouer la taille d'un objet plutôt que de le faire croître ou alors utiliser une liste. 

**Exercice**

On va programmer des fonctions dépendant d'un paramètre `n`.  

* **Méthode 1**:
    + Créer un vecteur `vec` vide de taille `n` (vous pouvez utiliser `NULL` ou `c()`)
    + Faire une boucle for de 1 à n ajoutant pour concatener l'élément `i` à `vec`

<div class="fold s">
```{r}
method1 = function(n) {
  vec = NULL # Or vec = c()
  for(i in seq_len(n))
    vec = c(vec, i)
  return(vec)
}
```
</div>

* **Méthode 2**:
    + Créer un vecteur de taille `n` en utilisant la fonction `numeric`
    + Assigner l'élément `i` dans `x[i]`

<div class="fold s">
```{r}
method2 = function(n) {
  vec = numeric(n)
  for(i in seq_len(n))
    vec[i] = i
  vec
}
```
</div>

* **Method 3**: créer une suite avec la fonction `seq_len`

<div class="fold s">
```{r}
method3 = function(n) seq_len(n)
```
</div>

* Comparer la performance des trois méthodes avec `microbenchmark::microbenchmark` avec $n=1000$: regarder les temps d'exécution en nanoseconds sur 100 répétitions des trois méthodes puis faire le graphique avec `ggplot2::autoplot`.

<div class="fold s">
```{r}
n <- 1e4
micro <- microbenchmark::microbenchmark(times = 100, 
                                        method1(n), method2(n), method3(n))
print(micro)
ggplot2::autoplot(micro)
```
</div>

* Comparer le poids final du vecteur de sortie de `method1` avec `n=1e5` et le besoin mémoire pour le générer avec `profvis::profvis` (cliquez sur l'onglet `data`)

<div class="fold s">
```{r, results="hide"}
n <- 1e5
profvis::profvis(x <- method1(n))
pryr::object_size(x)
```
</div>

La méthode optimale est bien-sûr celle utilisant la fonction adaptée. La méthode 2, loin d'être efficiente, est 200 fois plus rapide que la méthode 1. @gillespie2016efficient propose plus de résultats en faisant varier la taille du vecteur. Quand $n=10^7$, la méthode 1 prend une heure quand la méthode 2 fonctionne en 2 secondes et la méthode 3 est instantanée. En termes de consommation de RAM, le *profiling* nous montre que la méthode 1 est très gourmande: pour générer un vecteur d'une taille finale de 400kB, il faut environ 1.3 Gigas de RAM qu'on rend ensuite au système. 


La pré-allocation est une manière de réduire les besoins mémoires et donc d'accélérer le code. Une autre solution est l'utilisation de listes, qui peut être pratique lorsqu'on ne connaît pas la taille d'un élément à l'avance. Les listes, contrairement aux vecteurs ou matrices, stockent leurs éléments dans des emplacements mémoires non contigües ce qui permet d'ajouter un élément sans copier le reste de l'objet et permet ainsi la modification par passage.  




### Conclusion

**TO DO**


### References



<!------------------------

# Example: Monte-Carlo integration

It’s also important to make full use of R functions that use vectors. For example, suppose we wish to estimate the integral 

$$
\int_0^1 x^2dx
$$

using a Monte-Carlo method. Essentially, we throw darts at the curve and count the number of darts that fall below the curve

Monte Carlo Integration

```{r, eval = FALSE}

Initialise: hits = 0
for i in 1:N:
  Generate two random numbers, U1,U2, between 0 and 1
  If U2<U21,
    then hits = hits + 1
end for

Area estimate = hits/N
```

Implementing this Monte-Carlo algorithm in `R` would typically lead to something like:

```{r}
monte_carlo = function(N) {
  hits = 0
  for (i in seq_len(N)) {
    u1 = runif(1)
    u2 = runif(1)
    if (u1 ^ 2 > u2)
      hits = hits + 1
  }
  return(hits / N)
}
```

In R this takes a few seconds:

```{r}
N = 500000
system.time(monte_carlo(N))
```

In contrast a more R-centric approach would be

```{r}
monte_carlo_vec = function(N) sum(runif(N)^2 > runif(N))/N
```

The monte_carlo_vec() function contains (at least) four aspects of vectorisation

1. The runif() function call is now fully vectorised;
1. We raise entire vectors to a power via ^;
1. Comparisons using > are vectorised;
1. Using sum() is quicker than an equivalent for loop.

The function `monte_carlo_vec()` is around 30 times faster than monte_carlo():
```{r}
N = 500000
system.time(monte_carlo_vec(N))
```
----------->
