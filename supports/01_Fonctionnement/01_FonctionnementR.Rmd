---
title: "Principes: pourquoi et comment optimiser en R?"
author: "Lino Galiana"
date: "14 juin 2019"
output: html_document
bibliography: ../bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

>  Premature optimization is the root of all evil (or at least most of it) in programming.
>
> Donald Knuth

# Introduction

L'objectif du cours est d'offrir une introduction aux possibilités qu'offre `R` pour faire des calculs lourds et lire de grosses bases de données. Les deux principales ressources disponibles sur le sujet sont les ouvrages *Efficient R Programming* [@gillespie2016efficient] et *Advanced R* [@wickham2014advanced]^[Dans ce cours, on adoptera la vision de l'efficience au sens d'efficience algorithmique, i.e. la vitesseà laquelle un programme s'exécute et les ressources nécessaires pour l'accomplissement de celui-ci. Nous parlerons peu de la notion d'efficacité de programmation: quantité de travail utile qu'un programmeur peut réaliser par unité de temps. A cet égard, des astuces et règles utiles sont présentées dans la Formation *travail collaboratif sous R* et dans le livre de @gillespie2016efficient].

Avant de chercher à optimiser, le premier réflexe à adopter est de bien structurer le problème que l'on vise à résoudre et sélectionner le format de données et les *packages* les plus adéquats. `R` étant un langage très malléable, il existe généralement plusieurs manières de résoudre un problème: avoir un code efficace nécessite en premier lieu de choisir le meilleur outil disponible en `R`. Par exemple, si on doit passer par une boucle, il vaut mieux choisir rapidement de faire du `Rcpp` plutôt qu'écrire une boucle `R` qu'on accélèrera par la suite. Un des objectifs de ce cours est de faire connaître les outils les plus adéquats pour des problèmes génériques. Plutôt qu'essayer de paralléliser sur 16 coeurs, il est parfois préférable de bien en utiliser un seul. 

Quand on se préoccupe d'efficacité ou de gestion de mémoire, le principal risque est la dette technique: un code pas très bon pour lequel on reporte le moment de l'améliorer. Il s'agit, par exemple, d'un code qui utilise des boucles au lieu d'une opération vectorisée. Bien réfléchir à la structure du programme, à son découpage en parties homogènes, au format de données et aux packages mobilisables, est fondamental.

Alors que le volume de données disponibles et pouvant être combinées est croissant, le ralentissement des progrès technologiques dans l'industrie des microprocesseurs (ralentissement de la loi de Moore^[Selon cette loi, le nombre de transistors composant un micro-processeur double tous les 18 mois à coût constant. Cela implique une capacité de calcul croissant à une grande vitesse. Cette observation empirique a été globalement respectée jusqu'en 2016, date depuis laquelle les progrès ont marqué le pas.]) justifie l'utilisation croissante de capacités de calculs parallèles. L'exploitation des systèmes multi-coeurs, qui sont maintenant communs, permet de limiter l'effet limitant du ralentissement de la loi de Moore sur les capacités de traitement. 

# Plan du cours

La première étape nécessaire pour être en mesure d'être efficace en `R` et de comprendre le fonctionnement du langage. Comprendre la manière dont `R` fonctionne permet de comprendre l'intérêt de la vectorisation ainsi que la manière dont la mémoire fonctionne en `R`. Ce détour par le lange de base permettra de mieux comprendre les raisons pour lesquelles `R` est un langage plus lent qu'un langage compilé et limité lorsqu'il est question de données volumineuses.


# Outils utiles pour mesurer l'exécution de fonctions

Nous allons utiliser deux types d'approches pour minuter la vitesse d'exécution d'un programme:

* **Benchmarking**: test de performance basé sur la répétition d'opérations
* **Profiling**: exécution d'une série de commande afin de déterminer les goulets d'étranglement

La première approche est généralement suivie lorsqu'on veut optimiser une opération pour laquelle plusieurs méthodes existent. La seconde approche permet de cibler les étapes d'un programme à cibler pour accélérer les calculs ou réduire l'utilisation de la mémoire.


## Microbenchmark

La fonction `system.time()` est la manière la plus simple de mesurer le temps d'éxécution d'une ou plusieurs fonctions. Cependant, le temps d'exécution observé n'est qu'une réalisation d'une variable aléatoire et dépend de l'utilisation, à l'instant *t*, des ressources d'un ordinateur. Pour obtenir une image plus fidèle de la vitesse d'exécution d'une fonction, et être en mesure de comparer plus pleinement des temps d'exécution, le package `microbenchmark` est très pratique. Comme les fonctions évaluées sont répétées de nombreuses fois (par défaut 100), on utilise `microbenchmark` pour des opérations rapides (quitte à utiliser les fonctions évaluées sur un échantillon plus petit de données). 

Par exemple, si on désire évaluer la performance des moyennes par groupe avec `dplyr`, `data.table` et en base `R`

<!-----
Exercice: Utiliser le package microbenchmark pour faire une moyenne par groupe du dataframe suivant

1. Avec une solution `R` base:  `aggregate`
2. Avec une solution `dplyr`: `group_by` puis `summarise`
3. Avec une solution `data.table`: `dt[,mean(),by = ]`
----->

```{r first microbenchmark example, message = FALSE}
# On importe seulement le pipe, pas tout dplyr
import::from("magrittr","%>%")

# On crée un dataframe avec 5 catégories
df <- data.frame(x = rnorm(10e5),
                 y = sample(1:5,size = 10e5,
                            replace = TRUE))
# On crée son alter-ego data.table pour éviter de faire la conversion N fois
dt <- data.table::data.table(df)
# On indexe le data.table pour tirer pleinement parti du package (cf. chapitre data.table)
data.table::setkeyv(dt,"y")



# Compare data.table, dplyr, base R
micro <- microbenchmark::microbenchmark(
  aggregate(df$x, by = list(df$y), FUN = mean),
  dt[,mean(x), by = y],
  df %>% dplyr::group_by(y) %>% dplyr::summarise(mean(x)),
  times = 20
)
print(micro)

ggplot2::autoplot(micro)
```

Cela nous permet de voir que l'approche `data.table` est plus rapide (l'ordre de grandeur est ici de l'ordre de 1 pour 4 par rapport à `dplyr` et 1 pour 40 par rapport à la solution R base.)

## Profiling

Le *profiling* consiste à contrôler le temps d'exécution et l'usage mémoire d'une série de commandes. C'est une approche particulièrement bien adaptée à des codes modulaires (découpés sous formes de *chunks*) puisqu'on peut vraiment contrôler les parties de code les plus gourmandes en temps ou en mémoire. 


```{r, eval = FALSE}
profvis::profvis(expr = {
  
  # Stage 1: load packages
  # library("rnoaa") # not necessary as data pre-saved
  library("magrittr")
  
  # Stage 2: load and process data
  df <- data.frame(x = rnorm(10e5),
                    y = sample(1:5,size = 10e5, replace = TRUE))
  
  df2 <- df %>% dplyr::group_by(y) %>%
    dplyr::summarise(x2 = mean(x)) %>%
    dplyr::mutate(x2 = x2/mean(x2))
  
  df <- df %>% dplyr::left_join(df2)

  # Imaginons on fait une moyenne par groupe avec lapply
  # (nb: pas du tout la methode la plus efficace)
  # lapply(unique(df$y))

  # Stage 3: visualise output
  p <- ggplot2::ggplot(df,
                  ggplot2::aes(x = factor(y), y = x)
                  ) +
    ggplot2::geom_boxplot(aes(colour = factor(y))) +
    ggplot2::theme_bw()
  
  print(p)
  
})


```

Avec cet exemple, on voit que la majorité du temps est passée dans la représentation graphique, étape la plus longue et la plus exigente en mémoire. 


## Exercice

```{r}
x = 1:100 # initiate vector to cumulatively sum

# Method 1: with a for loop (10 lines)
cs_for = function(x){
  for(i in x){
    if(i == 1){
      xc = x[i]
    } else {
      xc = c(xc, sum(x[1:i]))
    }
  }
  xc
}

# Method 2: with apply (3 lines)
cs_apply = function(x){
  sapply(x, function(x) sum(1:x))
}

# Method 3: cumsum (1 line, not shown)
print(
  microbenchmark::microbenchmark(cs_for(x), cs_apply(x), cumsum(x))
)

```

# Comprendre les ordinateurs pour comprendre `R`

## Benchmarkme

```{r, eval = FALSE}
res = benchmarkme::benchmark_std() 
```

and compare the results to other users

```{r, eval = FALSE}
plot(res)
```

![](./pics/benchmarkme1.png)
![](./pics/benchmarkme2.png)
![](./pics/benchmarkme3.png)

# Appels de fonctions

Ultimately calling an `R` function always ends up calling some underlying `C`/`Fortran` code. For example the base `R` function `runif()` only contains a single line that consists of a call to `C_runif()`

```{r}
runif
```

A golden rule in `R` programming is to access the underlying `C`/`Fortran` routines as quickly as possible; the fewer functions calls required to achieve this, the better.

For example, suppose `x` is a standard vector of length `n`. Then

```{r, eval = FALSE}
x = x + 1
```

involves a single function call to the `+` function. Whereas the `for` loop

```{r, eval = FALSE}
for(i in seq_len(n)) 
  x[i] = x[i] + 1 
```

has

* *n* function calls to `+`
* *n* function calls to the `[` function;
* *n* function calls to the `[<-` function (used in the assignment operation);
* Two other function calls: one to `for` and another to `seq_len()`.

It isn’t that the for loop is slow, rather it is because we have many more function calls. Each individual function call is quick, but the total combination is slow.

Use the `microbenchmark` package to compare the vectorised construct `x = x + 1`, to the for loop version. Try varying the size of the input vector.

```{r}
plus_one <- function(n){
  x <- runif(n)
  x <- x+1
  return("OK")
}

plus_one_for <- function(n){
  
  x <- runif(n)
  for(i in seq_len(n)) x[i] = x[i] + 1 
  return("OK")
  
  return(x)
}

results_plus_one <- microbenchmark::microbenchmark(
  plus_one(10^2),
  plus_one(10^3),
  plus_one(10^4),
  plus_one(10^5),
  plus_one_for(10^2),
  plus_one_for(10^3),
  plus_one_for(10^4),
  plus_one_for(10^5),
  times = 50
  )
ggplot2::autoplot(results_plus_one)

```


### Memory allocation

Another general technique is to be careful with memory allocation. If possible pre-allocate your vector then fill in the values

You should also consider pre-allocating memory for data frames and lists. Never grow an object. A good rule of thumb is to compare your objects before and after a `for` loop; have they increased in length? 

Let’s consider three methods of creating a sequence of numbers. **Method** 1 creates an empty vector and gradually increases (or grows) the length of the vector

```{r}
method1 = function(n) {
  vec = NULL # Or vec = c()
  for(i in seq_len(n))
    vec = c(vec, i)
  vec
}
```

**Method** 2 creates an object of the final length and then changes the values in the object by subscripting:

```{r}
method2 = function(n) {
  vec = numeric(n)
  for(i in seq_len(n))
    vec[i] = i
  vec
}
```

**Method 3** directly creates the final object

```{r}
method3 = function(n) seq_len(n)
```

To compare the three methods we use the `microbenchmark()` function from the previous chapter

```{r}
n <- 1000
ggplot2::autoplot(
  microbenchmark::microbenchmark(times = 100, unit = "s", 
                                 method1(n), method2(n), method3(n))
)
```

The table below shows the timing in seconds on my machine for these three methods for 100 run of those methods for a 1000 observations. The relationships for varying *n* are all roughly linear on a log-log scale, but the timings between methods are drastically different. Notice that the timings are no longer trivial. When $n=10^7$, method 1 takes around an hour whilst method 2 takes 2 seconds and method 3 is almost instantaneous. Remember the golden rule; access the underlying `C`/`Fortran` code as quickly as possible.


### Vectorised code

Recall the golden rule in R programming, access the underlying C/Fortran routines as quickly as possible; the fewer functions calls required to achieve this, the better. With this mind, many R functions are vectorised, that is the function’s inputs and/or outputs naturally work with vectors, reducing the number of function calls required. For example, the code

```{r, eval = TRUE}
n <- 10^4
x = runif(n) + 1
```

performs two vectorised operations. 

1. First runif() returns n random numbers.
2. Second we add 1 to each element of the vector. In general it is a good idea to exploit vectorised functions.

Consider this piece of R code that calculates the sum of `log(x)`

```{r, eval = TRUE}
log_sum = 0
for(i in 1:length(x))
  log_sum = log_sum + log(x[i])
```

This code could easily be vectorised via

```{r}
log_sum = sum(log(x))
```

Writing code this way has a number of benefits.

1. It’s faster. When n=107, the `R` way is about forty times faster.
2. It’s neater.
3. It doesn’t contain a bug when x is of length 0


As with the general example in section 3.2, the slowdown isn’t due to the for loop. Instead, it’s because there are many more functions calls.

#### Example: Monte-Carlo integration

It’s also important to make full use of R functions that use vectors. For example, suppose we wish to estimate the integral 

$$
\int_0^1 x^2dx
$$

using a Monte-Carlo method. Essentially, we throw darts at the curve and count the number of darts that fall below the curve

Monte Carlo Integration

```{r, eval = FALSE}

Initialise: hits = 0
for i in 1:N:
  Generate two random numbers, U1,U2, between 0 and 1
  If U2<U21,
    then hits = hits + 1
end for

Area estimate = hits/N
```

Implementing this Monte-Carlo algorithm in `R` would typically lead to something like:

```{r}
monte_carlo = function(N) {
  hits = 0
  for (i in seq_len(N)) {
    u1 = runif(1)
    u2 = runif(1)
    if (u1 ^ 2 > u2)
      hits = hits + 1
  }
  return(hits / N)
}
```

In R this takes a few seconds:

```{r}
N = 500000
system.time(monte_carlo(N))
```

In contrast a more R-centric approach would be

```{r}
monte_carlo_vec = function(N) sum(runif(N)^2 > runif(N))/N
```

The monte_carlo_vec() function contains (at least) four aspects of vectorisation

1. The runif() function call is now fully vectorised;
1. We raise entire vectors to a power via ^;
1. Comparisons using > are vectorised;
1. Using sum() is quicker than an equivalent for loop.

The function `monte_carlo_vec()` is around 30 times faster than monte_carlo():
```{r}
N = 500000
system.time(monte_carlo_vec(N))
```


# Parallélisation

Parallel computing refers to situations where calculations are carried out simultaneously, for example distributing the calculations across multiple cores of your computer’s processor, as opposed to having the calculations run sequentially on a single core. Parallel computing is particularly suitable for ‘single program, multiple data’ problems, for example in simulations and bootstrapping.

Un processeur est dit multithread s'il est capable d'exécuter efficacement plusieurs threads simultanément. Contrairement aux systèmes multiprocesseurs (tels les systèmes multi-cœur), les threads doivent partager les ressources d'un unique cœur1 : les unités de traitement, le cache processeur et le translation lookaside buffer ; certaines parties sont néanmoins dupliquées : chaque thread dispose de ses propres registres et de son propre pointeur d'instruction. Là où les systèmes multiprocesseurs incluent plusieurs unités de traitement complètes, le multithreading a pour but d'augmenter l'utilisation d'un seul cœur en tirant profit des propriétés des threads et du parallélisme au niveau des instructions. Comme les deux techniques sont complémentaires, elles sont parfois combinées dans des systèmes comprenant de multiples processeurs multithreads ou des processeurs avec de multiples cœurs multithreads. 

la librairie ne permet pas d’accélérer miraculeusement une procédure existante. En revanche, elle nous donne l’opportunité d’exploiter efficacement les ressources machines à condition de reprogrammer la procédure en réorganisant judicieusement les calculs. L’idée maîtresse est de pouvoir décomposer le processus en tâches que l’on peut exécuter en parallèle, charge à nous par la suite d’effectuer la consolidation.

Both multithreading and multi-core approaches exploit the concurrency in a computational workload. 

More subtly, if a load that would saturate a 1GHz processor could be spread evenly across 4 processors, those processors could be run at roughly 250MHz each. If each 250MHz processor is less than a quarter the size of the 1GHz processor, or consumes less than a quarter the power (either of which may be the case because of the nonlinear cost of higher operating frequencies), the multi-core system might be more economical. 

 Multithreaded processors also exploit the concurrency of multiple tasks, but in a different way and for a different reason. Instead of a system-level technique to spread CPU load, multithreading is processor-level optimization to improve area and energy efficiency. Multithreaded architecture is driven to a large degree by the realization that single-threaded, high-performance processors spend a surprising amount of time doing nothing. When the results of a memory access are required for a program to advance, and that access must reference RAM whose cycle time is tens of times slower than that of the processor, a single-threaded processor can do nothing but stall until the data is returned.

Multithreading can be described thus: If latencies prevent a single task from keeping a processor pipeline busy, a single pipeline should be able to complete more than one concurrent task in less time than it would take to run the tasks serially. This means running more than one task's instruction stream, or thread, at a time, which in turn means that the processor has to have more than one program counter and more than one set of programmable registers. Replicating those resources is far less costly than replicating an entire processor. 

You can have multithreading with a single cpu and single core

A thread of execution is the smallest sequence of programmed instructions that can be managed independently by an operating system scheduler
On thing on threads the operating system performs a task known as "scheduling" in which it tries to find the optimal method of distributing workloads across available resources. 

Multithreading and Multicore are different pieces of terminology that apply to different areas of computing.

    Multicore refers to a computer or processor that has more than one logical CPU core, and that can physically execute multiple instructions at the same time. A computer's "core count" is the total number of cores the computer has: computers may have multiple processors, each of which might have multiple cores; the core count is the total number of cores on all of the processors.

    Multithreading refers to a program that can take advantage of a multicore computer by running on more than one core at the same time. In general, twice as many cores equals twice as much computing power (for programs that support multithreading) though some problems are limited by factors other than CPU usage; these problems will not experience gains that are as dramatic from multithreading.
    
a running thread will consume all of a core. The only reason your CPU isnt running at 100% all the time is that the operating system knows how to suspend the processor, which basically makes it stop everything and wait until something happens (such as an IO event or a clock tick). Only one thread can run on a core at once. Different threads running is actually just threads jumping onto the CPU and running for short periods of time, then being switched out with other threads which also need to run.

The number of cores and number of threads are decoupled. You can have many threads running on a single core, and you can have situations where only one thread runs despite the presence of more cores



The foreach package must be used in conjunction
with a package such as doParallel in order to execute code in parallel. The user must register a
parallel backend to use, otherwise foreach will execute tasks sequentially, even when the %dopar%
operator is used

y default, doParallel uses multicore
functionality on Unix-like systems and snow functionality on Windows. Note that the multicore
functionality only runs tasks on a single computer, not a cluster of computers. However, you can
use the snow functionality to execute on a cluster, using Unix-like operating systems, Windows, or
even a combination. It is pointless to use doParallel and parallel on a machine with only one
processor with a single core. To get a speed improvement, it must run on a machine with multiple
processors, multiple cores, or both.

For non-Linux users, we can use parLapply function in parallel package to achieve parallelism. parLapply function supports different platforms including Windows, Linux and Mac with better portability, but its usage is a little complicated than mclapply. Before using parLapply function, we need to create a computing group (cluster) first. Computing group is a software-level concept, which means how many R worker processes we need to create (Note: par*apply package will create several new R processes rather than copies of R master process from mc*apply). Theoretically, the size of the computing group is not affected by the hardware configuration.For example, we can create a group with 1000 R worker processes on any machine. In practice, we usually use the same size of computing group with hardware resources (such as physical cores) so that each worker process of R can be mapped to a physical core.
In the following example, we start with detectCores function to determine the number of computing cores in the machine.It is noteworthy that detectCores() returns the number of Hyper-Threading rather than real physical cores.For example, there are two physical cores on my laptop, and each core can simulate two hyperthreading , so detectCores() return value is 4. However, for many compute-intensive tasks, the Hyper-Threading is not much helpful for improving performance, so we use the parameter of logical=FALSE to get the actual number of physical cores and then create the same number group.Since the worker processes in the group is new R sessions, the data and functions of the parent process is not visible. Therefore, we have to broadcast the data and functions to all worker processes by clusterExport function. Finally parLapply will distribute the tasks to all R worker processes evenly, and then gather results back.
1
2
3
4
5
6
7
8
	
# cluster on Windows
cores <- detectCores(logical = FALSE)
cl <- makeCluster(cores)
clusterExport(cl, c('solve.quad.eq', 'a', 'b', 'c'))
system.time(
   res1.p <- parLapply(cl, 1:len, function(x) { solve.quad.eq(a[x], b[x], c[x]) })
)
stopCluster(cl)

For the for loop parallelization, we can use %dopar% in foreach package to distribute the computations to multiple R workers. foreach package provides a method of data mapping, but does not include the establishment of computing group.Therefore, we need to create a computing group by doParallel or doMC package. Creating computing group is as same as before, except setting backend of computations by registerDoParallel.

R was originally designed for single-threaded so that many of the underlying data structures and functions are not thread-safe. Therefore, lots of codes need to be rewritten or adjust for high-level parallel algorithms. 

# References