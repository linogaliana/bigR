---
title: "Parallélisation"
author: "Lino Galiana"
date: "14 juin 2019"
output: html_document
bibliography: ../bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The whole “let’s parallelize” thing is a huge waste of
everybody’s time. There’s this huge body of
“knowledge” that parallel is somehow more efficient,
and that whole huge body is pure and utter garbage.
Big caches are efficient. Parallel stupid small cores
without caches are horrible unless you have a very
specific load that is hugely regular (ie graphics).
[. . . ]
Give it up. The whole “parallel computing is the
future” is a bunch of crock.
Linus Torvalds,


Il faut pourtant conster que l'utilisation de codes compilés n'est plus suffisante pour obtenir des performances maximales. [...] Les fabricants ont cessé de chercher à créer des processeurs plus rapides, il y a environ 10 ans, lorsqu'ils ont plutôt choisi d'augmenter le nombre de coeurs computationnels à l'intérieur des processeurs. L'exploitation de ces coeurs multiples exige pour le programmeur l'adoption de techniques de programmation parallèle."
Vincent Miele et Violaine Louvet, Calcul parallèle dans R


Règle: ne jamais optimizer prématurément

https://books.google.fr/books?id=xaWyDgAAQBAJ&pg=PR9&lpg=PR9&dq=parallel+Rcpp+group+by&source=bl&ots=4CmQXgYd8v&sig=ACfU3U28I6J4EcQ4tx_SJi9QEjUnc7gMSw&hl=fr&sa=X&ved=2ahUKEwj5qcak2sjjAhUSJhoKHaxEDXUQ6AEwCHoECAkQAQ#v=onepage&q=parallel%20Rcpp%20group%20by&f=false


# Principe

On parle de calcul parallèle lorsque des calculs sont effectués de manière simultanée, par opposition à une exécution séquentielle des calculs. **embarassingly parallel*
Outre l'intérêt évident en termes de CPU, un autre intérêt du parallélisme est qu'il est plus parcimonieux en mémoire: en traitant des blocs moins volumineux, on réduit les besoins de mémoire (liés par exemple à des *copy* multiples)

Le calcul parallèle est particulièrement adapté dans des situations où on peut traiter des blocs de données indépendamment. Les simulations numériques, le bootstrapping ou encore des statistiques descriptives par groupe sont autant de champs où l'utilisation du calcul parallèle s'avère utile. La stratégie généralement adoptée est celle du `split`-`apply`-`combine` ou `divide and conquer`. On commence par découper (*split*) les données en blocs homogènes, indépendants. Sur ces blocs, on exécute (*apply*) des fonctions. Enfin, on regroupe les résultats (*combine*). On parle parfois de modèle `master`- `slave` ou `master`- `worker`: le maître (`master`) se charge des étapes *split* et *combine*, laissant l'étape *apply* aux esclaves (`slaves`). 

![](./pics/split_apply_combine.png)

Dans le scénario le plus simple, comme ci-dessous, la combinaison des données revient à une simple concaténation. Comme nous le verrons avec `spark`, l'étape finale peut être plus complexe.

Des language très performants, tels que `spark`, reposent sur ce principe de la parallélisation en traintant les données par bloc tout en assurant une communication efficace entre ces blocs (le paradigme `map-reduce` est une application du principe `split`-`apply`-`combine` aux données volumineuses, @wickham2011split). 

Paralléliser les opérations ne doit pas être le premier réflexe. Avant de se préoccuper de parallélisation, il convient de s'assurer que:

* le code est efficace sur un seul coeur
* le problème est adapté à la parallélisation. Le parallélisme ne prend sens que dans un cadre où on peut restructurer les données en blocs indépendants. Une somme cumulative sur un vecteur ne sera pas à une situation où la parallélisation apportera un gain (elle peut même ralentir le système). 
* le *hardware* est adapté à la parallélisation. Comme mettre en place une structure de parallélisation a un coup en termes de temps, il faut que le gain de temps associé à la parallélisation des calculs soit suffisant. De manière générale, il ne faut pas espérer une division du temps de calcul par $n$ dans un ordinateur avec $n$ coeurs. A partir de 4 coeurs, on peut tout de même espérer des gains de temps significatifs. 

Dans ce chapitre, nous montrerons les gains permis par la parallélisation en conjonction du `tidyverse`. Comme nous le montrerons dans le chapitre suivant, l'adoption de `data.table` offre une alternative intéressante et souvent préférable à une parallélisation aveugle. 


## Calcul parallèle vs multithreading

Un processeur est dit *multithread* s'il est capable d'exécuter efficacement plusieurs *threads* (tâches élémentaires) simultanément. Contrairement aux systèmes multiprocesseurs (tels les systèmes multi-coeur), les threads doivent partager les ressources d'un unique coeur : les unités de traitement, le cache processeur et le *translation look-aside buffer* ; certaines parties sont néanmoins dupliquées : chaque thread dispose de ses propres registres et de son propre pointeur d'instruction. Là où les systèmes multiprocesseurs incluent plusieurs unités de traitement complètes, le multithreading a pour but d'augmenter l'utilisation d'un seul coeur en tirant profit des propriétés des threads et du parallélisme au niveau des instructions. 

<!-------
La librairie ne permet pas d’accélérer miraculeusement une procédure existante. En revanche, elle nous donne l’opportunité d’exploiter efficacement les ressources machines à condition de reprogrammer la procédure en réorganisant judicieusement les calculs. L’idée maîtresse est de pouvoir décomposer le processus en tâches que l’on peut exécuter en parallèle, charge à nous par la suite d’effectuer la consolidation.


Multithreaded processors also exploit the concurrency of multiple tasks, but in a different way and for a different reason. Instead of a system-level technique to spread CPU load, multithreading is processor-level optimization to improve area and energy efficiency. Multithreaded architecture is driven to a large degree by the realization that single-threaded, high-performance processors spend a surprising amount of time doing nothing. When the results of a memory access are required for a program to advance, and that access must reference RAM whose cycle time is tens of times slower than that of the processor, a single-threaded processor can do nothing but stall until the data is returned.

Multithreading can be described thus: If latencies prevent a single task from keeping a processor pipeline busy, a single pipeline should be able to complete more than one concurrent task in less time than it would take to run the tasks serially. This means running more than one task's instruction stream, or thread, at a time, which in turn means that the processor has to have more than one program counter and more than one set of programmable registers. Replicating those resources is far less costly than replicating an entire processor. 

A thread of execution is the smallest sequence of programmed instructions that can be managed independently by an operating system scheduler
On thing on threads the operating system performs a task known as "scheduling" in which it tries to find the optimal method of distributing workloads across available resources. 

Multithreading and Multicore are different pieces of terminology that apply to different areas of computing.
---------->

## Fork vs socket



> Fork is a concept from POSIX operating systems, and should be available on all R platforms except Windows. This creates a new R process by taking a complete copy of the master process, including the workspace and state of the random-number stream.
> However, the copy will (in any reasonable OS) share memory pages with the master until modified so forking is very fast.
> Vignette du package `parallel`

Les systèmes `unix` (mac et linux) et windows ne gèrent pas la parallélisation de la même manière. Il y a en fait deux manières de gérer des opérations parallèles (plus de détail [ici](https://www.r-bloggers.com/parallel-r-socket-or-fork/) et dans la vignette du package `parallel`):

* **fork**: chaque processus parallèle est une duplication complète du maître et partage un environnement commun avec celui-ci, ce qui évite les copies d'objets réduisant la performance. Cette approche fonctionne uniquement dans les systèmes unix, pas windows.  
* **socket**: chaque processus fonctionne indépendamment, sans partage de variables ou objets communs sauf indication explicite (induisant une copie sur tous les slaves). Cela induit une perte de performance liée à la surcouche de communication. Néanmoins, c'est la seule approche possible sur Windows. 


```{r}
df <- split(iris, iris$Species)


ncores <- parallel::detectCores() - 1

ncores

parallel_group_by <- function(df,
                              ncores = ncores,
                              type = c("PSOCK", "FORK")){
  type <- match.arg(type)
  ncores <- parallel::detectCores() - 1
  cl <- parallel::makeCluster(ncores, type = type)  
  sp <- parallel::parLapply(cl, df, function(k) mean(k$Petal.Width, na.rm=TRUE))
  parallel::stopCluster(cl)
  return(sp)
}

microbenchmark::microbenchmark(
  parallel_group_by(df, type = "FORK")#,
#  parallel_group_by(df, type = "PSOCK")
)
```
  


# Parallélisation dans `R` : premiers essais

## Quel package utiliser ?

Il existe plusieurs packages dédiés à la parallélisation des calculs en `R`:

* `parallel`: pré-installé sur `R` depuis la version 2.14. Il s'agit de la fondation pour des packages ultérieurs. Cependant, la fonction `parallel::mclapply` fonctionne sur les systèmes `unix` (Mac et Linux) mais ne parallélise pas les calculs sur `Windows`. Sur `Windows`, on privilégiera `parallel::parLapply` qui fonctionne quelque soit l'OS et offre une syntaxe très proche de la fonction `lapply`
* `snow`: implémentation des fonctions `*apply` dans un cadre parallèle. La fonction `snow::parLapply` permet, par exemple, d'effectuer des traitements sur chaque éléments d'une liste de manière indépendante. 
* `foreach`: offre une plus grande flexibilité. C'est le package que nous utiliserons. Le package doit être utilisé en conjonction avec un package comme `doParallel` pour mettre en place les *clusters* et les associer aux données devant être traitées^[`doParallel` est approprié pour gérer la parallélisation sur un ordinateur individuel car il repose sur le package `multicore`. Il gère la différence entre un système windows et unix. En revanche, `doParallel` n'est pas adapté pour une parallélisation sur un cluster d'ordinateurs]. Sans initialisation des *clusters*, la commande `%dopar%` ne produira pas la parallélisation attendue mais exécutera les tâches séquentiellement. 

La parallélisation nécessite d'être attentif à l'initialisation des processus esclaves. Même si, en théorie, on peut fixer autant de processus que désiré, indépendamment du *hardware*, il est préférable, en pratique, de fixer le nombre de processus au nombre de coeurs disponibles^[Un ordinateur peut avoir plus de coeurs disponibles (coeurs logiques) que de coeurs physiques. On utilisera le nombre de coeurs logiques moins un. Pour utiliser le nombre de coeurs physiques d'une machine, on peut faire `parallel::detectCores(logical = FALSE)` au lieu de `parallel::detectCores()`] moins un. Le fait de créer $N-1$ processus esclaves permet de garder un coeur au *master*. 

<!-------
In the following example, we start with detectCores function to determine the number of computing cores in the machine.It is noteworthy that detectCores() returns the number of Hyper-Threading rather than real physical cores.For example, there are two physical cores on my laptop, and each core can simulate two hyperthreading , so detectCores() return value is 4. However, for many compute-intensive tasks, the Hyper-Threading is not much helpful for improving performance, so we use the parameter of logical=FALSE to get the actual number of physical cores and then create the same number group.Since the worker processes in the group is new R sessions, the data and functions of the parent process is not visible. Therefore, we have to broadcast the data and functions to all worker processes by clusterExport function. Finally parLapply will distribute the tasks to all R worker processes evenly, and then gather results back.

------->

# Parallélisation avec le package `foreach`

## Mise en place de la parallélisation

<!----
library(doParallel)
> cl <- makeCluster(2)
> registerDoParallel(cl)
------>

Le package foreach propose deux fonctions utiles pour le traitement d'opérations:

* `%do%`: propose une alternative à `lapply` pour un traitement séquentialisé des opérations
* `%dopar%`: propose une alternative à `lapply` pour un traitement parallélisé des opérations

Pour se simplifier la vie, on va importer le package `foreach`:

```{r}
library(foreach)
```

```{r}
microbenchmark::microbenchmark(
  foreach(i=1:100) %do% sqrt(i),
  foreach(i=1:100) %dopar% sqrt(i),
  sqrt(1:100)
)
```

Sans définition des clusters, la fonction `%dopar%` exécute ainsi les tâches séquentiellement: on ne peut espérer de gain de celle-ci par rapport à une solution non parallélisée. `%dopar%` renvoit d'ailleurs un *warning* dans ce type de situation. Ce benchmark montre, encore une fois, qu'une solution vectorisée reste préférable à une exécution séquentialisée sous-optimale. 


## Initialisation de la configuration parallèle

Il existe de multiples manières d'initialiser la parallélisation. Le modèle ci-dessous suit le plan suivant:

* on découpe les données en fonction d'une variable de groupe
* on initialise les processus esclaves et crée une barre de progrès
* on effectue une ou plusieurs tâches en parallèle
* on ferme les clusters. C'est important de le faire après chaque parallélisation pour éviter de saturer la mémoire et la CPU avec des processus inactifs


```{r, eval = FALSE}

df <- data.frame(
  x = rnorm(10e5),
  y = sample(1:10, size = 10e5, replace = TRUE)
)

df <- split(df,df$y)

# CLUSTER INITIALIZATION  
cl <- parallel::makeCluster(parallel::detectCores()-1,
                            outfile = "")
doSNOW::registerDoSNOW(cl)

# ON CREE LA BARRE DE PROGRES
pb <- txtProgressBar(max = length(df), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
#
# PARTIE PARALLELISATION
#
parallel::stopCluster(cl)
```


```{r, eval = FALSE}
stopCluster
```

## Un premier programme parallélisé

On reprend le *dataframe*

```{r}
df <- data.frame(
  x = rnorm(10e7),
  y = sample(1:50, size = 10e7, replace = TRUE)
)

df_list <- split(df,df$y)
```

On va créer les clusters et utiliser le package `foreach`. On fournit les options suivantes à `foreach`:

* g: le groupe sur lequel on effectue le traitement. Vous pouvez l'appeler autrement, c'est un argument qui n'est pas contraint par le nom tant que vous êtes cohérent avec celui-ci dans `%dopar%`
* .combine: la manière dont les données sont assemblées à l'issue des tâches. A titre personnel, j'utilise toujours `.combine = "list"` pour la flexibilité permise par `R` dans le maniement des listes. D'autres combinaisons sont possibles (`c`,`cbind`,`rbind`,`+`, etc. )
* `.multicombine = TRUE` et `.maxcombine = TRUE`: permet de sortir une liste, éventuellement à plusieurs niveaux, dont le nombre de premier niveau est égal au nombre de groupes
* `.options.snow`: les options supplémentaires à passer au package `foreach`. Ici, ce sont les barres de progrès


```{r}
profvis::profvis({
  # CLUSTER INITIALIZATION  
  cl <- parallel::makeCluster(parallel::detectCores()-1,
                              outfile = "")
  doSNOW::registerDoSNOW(cl)
  
  pb <- txtProgressBar(max = length(df_list), style = 3)
  progress <- function(n) setTxtProgressBar(pb, n)
  opts <- list(progress = progress)
  
  output <- foreach::foreach(g = df_list, .combine = "list",
                             .multicombine = TRUE,
                             .maxcombine = length(df_list),
                             .options.snow=opts
                             #                       .export = c("descript"),
                             #                       .packages = c("dplyr")
  ) %dopar% {
    data.frame(x = mean(g$x), y = unique(g$y))
  }
  
  # CLOSE CLUSTERS  
  parallel::stopCluster(cl)
  
  output <- do.call(rbind, output)
})
```

De nombreuses options supplémentaires sont disponibles, rendant le contrôle de la configuration aisé. La ligne finale `do.call(rbind, output)` consiste seulement à restructurer les données sous forme de *dataframe*: c'est un enchaînement classique et très pratique d'avoir des résultats stockés sous forme de liste et appliquer la fonction `do.call`


```{r}
profvis::profvis({
  import::from("magrittr","%>%")
  output <- df %>% dplyr::group_by(y) %>%
    dplyr::summarise(x = mean(x))
})
```

Ce *profiling* montre qu'ici l'approche parallélisée est moins rapide (le coût fixe payé par l'initialisation des clusters n'est pas compensé par un temps d'exécution plus rapide que `dplyr`). Néanmoins, en termes de mémoire, la parallélisation est beaucoup plus parcimonieuse. Le fait de traiter des données en blocs moins volumineux permet d'utiliser 20 fois moins de mémoire qu'avec `dplyr`. 

Voyons voir l'intérêt de la parallélisation sur une tâche plus complexe: une régression linéaire par groupe 

```{r}
df <- data.frame(
  x = rnorm(10e6),
  z = sample(1:50, size = 10e6, replace = TRUE)
)

df$y <- df$x + rnorm(10e6)

df_list <- split(df,df$z)
```

Les deux méthodes en compétition sont les suivantes:

```{r}
profvis::profvis({
  import::from("magrittr","%>%")
  output <- df %>% dplyr::group_by(z) %>%
    dplyr::do(ols = coef(lm(y ~x, data = .))) %>%
    tidyr::unnest()
})
```

```{r}
profvis::profvis({
  # CLUSTER INITIALIZATION  
  cl <- parallel::makeCluster(parallel::detectCores()-1,
                              outfile = "")
  doSNOW::registerDoSNOW(cl)
  
  pb <- txtProgressBar(max = length(df_list), style = 3)
  progress <- function(n) setTxtProgressBar(pb, n)
  opts <- list(progress = progress)
  
  output <- foreach::foreach(g = df_list, .combine = "list",
                             .multicombine = TRUE,
                             .maxcombine = length(df_list),
                             .options.snow=opts
                             #                       .export = c("descript"),
                             #                       .packages = c("dplyr")
  ) %dopar% {
    return(c(unique(g$z), coef(lm(y ~x, data = g))))
  }
  
  # CLOSE CLUSTERS  
  parallel::stopCluster(cl)
  
  output <- do.call(rbind, output)
})
```

Cette fois, la solution parallélisée est légèrement plus rapide et demande 2000 fois moins de mémoire. Cela illustre bien la manière dont l'approche divide and conquer peut aider en cas de traitements gourmands en RAM à limiter l'inflation


## Export de dépendances

The solution, which worked for me, was using additional loop's parameters (.export and .packages) and pass the function and package names explicitly.

setting .combine parameter.


  # CLUSTER INITIALIZATION  
  cl <- parallel::makeCluster(parallel::detectCores()-1,
                              outfile = "")
  doSNOW::registerDoSNOW(cl)
  
  pb <- txtProgressBar(max = length(liste_tranche), style = 3)
  progress <- function(n) setTxtProgressBar(pb, n)
  opts <- list(progress = progress)
  
  
  df <- foreach::foreach(g = liste_tranche, .combine = "list",
                         .multicombine = TRUE,
                         .maxcombine = length(liste_tranche),
                         .options.snow=opts,
                         .export = c("descript"),
                         .packages = c("dplyr")) %dopar% {
                           referent_actif_groupe(g, descript)
                         }
  
  # CLOSE CLUSTERS  
  parallel::stopCluster(cl)
